{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "- This notebook will apply the cleaned dataset from homework 1 and use it to create 3 different models to predict whether an applicant for credit will repay their credit within 2 years.\n",
    "    - The value 0 indicates that an animal will have a positive outcome (adopted/returned to owner).\n",
    "    - The value 1 indicates that an animal will have a negative outcome (death).\n",
    "- Each model will be a supervised learning model and the output will be a binary classification. \n",
    "- For each applicant a number of features will be given (independent variables) and the target, risk performance (dependent variable), will be predicted.\n",
    "\n",
    "This homework will be broken down into 5 main parts:\n",
    "1. Review the dataset from homework one and decide on which features to use to build our model\n",
    "2. Create a Linear Regression model and analyse\n",
    "3. Create a Logistical Regression model and analyse\n",
    "4. Create a Random Forest model and analyse\n",
    "5. Try to optimize each model\n",
    "\n",
    "We will begin by importing the packages needed for this assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from patsy import dmatrices\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import auc\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "\n",
    "import graphviz\n",
    "from graphviz import Source\n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Data Understanding and Prep\n",
    "\n",
    "The original dataset was cleaned in homework 1 and will now be imported as a starting point for this homework. There are some points to note before proceeding: \n",
    "\n",
    "- The cleaned dataset will be a starting point however, some additional cleaning steps will be performed before proceeding with homework2. \n",
    "- The data quality report for the cleaned dataset has been provided as a reference. \n",
    "- A summary of the data quality plan is seen below. \n",
    "- Based on the findings in homework1, four additional features were added. These are: \n",
    "    - 'SexKnown' which indicates whether the sex of an animal is known or not. \n",
    "    - 'CatOrDog' which indicates whether an animal is either a cat or a dog or not. \n",
    "    - 'AgeIntake_bins which grouped the ages of animals upon intake into four equal frequency bins.\n",
    "    - 'SickOrInjured' which indicates whether an animal was sick/injured or not. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert data quality plan table here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will begin by importing the cleaned dataset from homework 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the cleaned csv\n",
    "df = pd.read_csv(\"19205514_cleaned_new_features_added.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data integrity checks\n",
    "\n",
    "Before going any further we will now perform some data integrity checks on the dataset which we have just imported. \n",
    "\n",
    "We will now begin by taking a look at the shape of the dataset and first five lines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 23)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#look at the shape of the dataset\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AnimalID</th>\n",
       "      <th>Name_Provided</th>\n",
       "      <th>DateTime_Intake</th>\n",
       "      <th>FoundLocation</th>\n",
       "      <th>IntakeType</th>\n",
       "      <th>IntakeCondition</th>\n",
       "      <th>AnimalType_Intake</th>\n",
       "      <th>SexuponIntake</th>\n",
       "      <th>AgeuponIntake</th>\n",
       "      <th>Breed_Intake</th>\n",
       "      <th>...</th>\n",
       "      <th>SexuponOutcome</th>\n",
       "      <th>AgeuponOutcome</th>\n",
       "      <th>binary_outcome</th>\n",
       "      <th>ColorOriginal</th>\n",
       "      <th>BreedOriginal</th>\n",
       "      <th>percent</th>\n",
       "      <th>SexKnown</th>\n",
       "      <th>CatOrDog</th>\n",
       "      <th>AgeIntake_bins</th>\n",
       "      <th>SickOrInjured</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A687076</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2014-08-30 17:55:00</td>\n",
       "      <td>Austin (TX)</td>\n",
       "      <td>Stray</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Cat</td>\n",
       "      <td>Intact Female</td>\n",
       "      <td>76</td>\n",
       "      <td>Mixed Breed</td>\n",
       "      <td>...</td>\n",
       "      <td>Spayed Female</td>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "      <td>Torbie</td>\n",
       "      <td>Domestic Shorthair Mix</td>\n",
       "      <td>0.109769</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>(55.8, 92.0]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A685139</td>\n",
       "      <td>No</td>\n",
       "      <td>2014-08-03 11:23:00</td>\n",
       "      <td>Austin (TX)</td>\n",
       "      <td>Stray</td>\n",
       "      <td>Other</td>\n",
       "      <td>Dog</td>\n",
       "      <td>Intact Female</td>\n",
       "      <td>92</td>\n",
       "      <td>Mixed Breed</td>\n",
       "      <td>...</td>\n",
       "      <td>Spayed Female</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>White/Black</td>\n",
       "      <td>Cavalier Span Mix</td>\n",
       "      <td>0.109769</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>(55.8, 92.0]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A741039</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2016-12-27 11:18:00</td>\n",
       "      <td>Austin (TX)</td>\n",
       "      <td>Stray</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Dog</td>\n",
       "      <td>Intact Male</td>\n",
       "      <td>366</td>\n",
       "      <td>Pure Breed</td>\n",
       "      <td>...</td>\n",
       "      <td>Intact Male</td>\n",
       "      <td>371</td>\n",
       "      <td>0</td>\n",
       "      <td>Black/White</td>\n",
       "      <td>Jack Russell Terrier</td>\n",
       "      <td>0.109769</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>(365.0, 427.6]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A759166</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2017-09-28 11:03:00</td>\n",
       "      <td>Austin (TX)</td>\n",
       "      <td>Stray</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Cat</td>\n",
       "      <td>Intact Male</td>\n",
       "      <td>62</td>\n",
       "      <td>Pure Breed</td>\n",
       "      <td>...</td>\n",
       "      <td>Neutered Male</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>Black</td>\n",
       "      <td>Domestic Shorthair</td>\n",
       "      <td>0.109769</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>(55.8, 92.0]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A696479</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2015-02-05 11:51:00</td>\n",
       "      <td>Travis (TX)</td>\n",
       "      <td>Stray</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Cat</td>\n",
       "      <td>Intact Male</td>\n",
       "      <td>365</td>\n",
       "      <td>Mixed Breed</td>\n",
       "      <td>...</td>\n",
       "      <td>Neutered Male</td>\n",
       "      <td>375</td>\n",
       "      <td>0</td>\n",
       "      <td>Black</td>\n",
       "      <td>Domestic Medium Hair Mix</td>\n",
       "      <td>0.109769</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>(212.0, 365.0]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  AnimalID Name_Provided      DateTime_Intake FoundLocation IntakeType  \\\n",
       "0  A687076           Yes  2014-08-30 17:55:00   Austin (TX)      Stray   \n",
       "1  A685139            No  2014-08-03 11:23:00   Austin (TX)      Stray   \n",
       "2  A741039           Yes  2016-12-27 11:18:00   Austin (TX)      Stray   \n",
       "3  A759166           Yes  2017-09-28 11:03:00   Austin (TX)      Stray   \n",
       "4  A696479           Yes  2015-02-05 11:51:00   Travis (TX)      Stray   \n",
       "\n",
       "  IntakeCondition AnimalType_Intake  SexuponIntake  AgeuponIntake  \\\n",
       "0          Normal               Cat  Intact Female             76   \n",
       "1           Other               Dog  Intact Female             92   \n",
       "2          Normal               Dog    Intact Male            366   \n",
       "3          Normal               Cat    Intact Male             62   \n",
       "4          Normal               Cat    Intact Male            365   \n",
       "\n",
       "  Breed_Intake  ... SexuponOutcome AgeuponOutcome binary_outcome  \\\n",
       "0  Mixed Breed  ...  Spayed Female             82              0   \n",
       "1  Mixed Breed  ...  Spayed Female             96              0   \n",
       "2   Pure Breed  ...    Intact Male            371              0   \n",
       "3   Pure Breed  ...  Neutered Male             99              0   \n",
       "4  Mixed Breed  ...  Neutered Male            375              0   \n",
       "\n",
       "  ColorOriginal             BreedOriginal   percent SexKnown CatOrDog  \\\n",
       "0        Torbie    Domestic Shorthair Mix  0.109769        1        1   \n",
       "1   White/Black         Cavalier Span Mix  0.109769        1        1   \n",
       "2   Black/White      Jack Russell Terrier  0.109769        1        1   \n",
       "3         Black        Domestic Shorthair  0.109769        1        1   \n",
       "4         Black  Domestic Medium Hair Mix  0.109769        1        1   \n",
       "\n",
       "   AgeIntake_bins  SickOrInjured  \n",
       "0    (55.8, 92.0]              0  \n",
       "1    (55.8, 92.0]              0  \n",
       "2  (365.0, 427.6]              0  \n",
       "3    (55.8, 92.0]              0  \n",
       "4  (212.0, 365.0]              0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the feature 'percent' is still in the dataset. This was generating while implementing bar plots in homework1 and should not be part of the dataset. We will now drop this feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the feature 'percent'\n",
    "df = df.drop(\"percent\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, in order to conduct some integrity tests, we will look at the datatypes of our features after importing the csv. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnimalID             object\n",
       "Name_Provided        object\n",
       "DateTime_Intake      object\n",
       "FoundLocation        object\n",
       "IntakeType           object\n",
       "IntakeCondition      object\n",
       "AnimalType_Intake    object\n",
       "SexuponIntake        object\n",
       "AgeuponIntake         int64\n",
       "Breed_Intake         object\n",
       "Color_Intake         object\n",
       "DateTime_Outcome     object\n",
       "DateofBirth          object\n",
       "SexuponOutcome       object\n",
       "AgeuponOutcome        int64\n",
       "binary_outcome        int64\n",
       "ColorOriginal        object\n",
       "BreedOriginal        object\n",
       "SexKnown              int64\n",
       "CatOrDog              int64\n",
       "AgeIntake_bins       object\n",
       "SickOrInjured         int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the datatpyes of some features have reverted back to type \"object\". We will now convert these features back to their appropriate types. \n",
    "\n",
    "Although the target feature *binary_outcome* is a categorical feature, will need to be an int when creating scatter plots in the next section, so we can leave it as it is at this stage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert all objects to categories\n",
    "object_columns = df.select_dtypes(['object']).columns\n",
    "for column in object_columns:\n",
    "    df[column] = df[column].astype('category')\n",
    "\n",
    "#convert animal ID to an object type\n",
    "df['AnimalID'] = df['AnimalID'].astype('object')  \n",
    "\n",
    "#convert the new features 'SexKnown','CatOrDog' and 'SickOrInjured' back to category type as this \n",
    "#is how they were implemented in homework1.\n",
    "new_features = ['SexKnown','CatOrDog', 'SickOrInjured']\n",
    "for column in new_features:\n",
    "    df[column] = df[column].astype('category')\n",
    "    \n",
    "#convert all date features to datetime types\n",
    "date_columns = ['DateTime_Intake', 'DateTime_Outcome', 'DateofBirth']\n",
    "for column in date_columns: \n",
    "    df[column] = df[column].astype('datetime64[ns]')\n",
    "    \n",
    "#convert binary outcome to category type\n",
    "# df['binary_outcome'] = df['binary_outcome'].astype('category') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnimalID                     object\n",
       "Name_Provided              category\n",
       "DateTime_Intake      datetime64[ns]\n",
       "FoundLocation              category\n",
       "IntakeType                 category\n",
       "IntakeCondition            category\n",
       "AnimalType_Intake          category\n",
       "SexuponIntake              category\n",
       "AgeuponIntake                 int64\n",
       "Breed_Intake               category\n",
       "Color_Intake               category\n",
       "DateTime_Outcome     datetime64[ns]\n",
       "DateofBirth          datetime64[ns]\n",
       "SexuponOutcome             category\n",
       "AgeuponOutcome                int64\n",
       "binary_outcome                int64\n",
       "ColorOriginal              category\n",
       "BreedOriginal              category\n",
       "SexKnown                   category\n",
       "CatOrDog                   category\n",
       "AgeIntake_bins             category\n",
       "SickOrInjured              category\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to perform some data integrity tests. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test 1: Check if the 'DateTime_Outcome' is an earlier date than the 'DateTime_Intake' for any of the animals. This would imply that the animal stayed a negative length of time in the shelter which is impossible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows failing the test:  0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DateTime_Outcome</th>\n",
       "      <th>DateTime_Intake</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [DateTime_Outcome, DateTime_Intake]\n",
       "Index: []"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a dataframe containing the instances where date of outcome is less than date of intake\n",
    "test_1 = df[['DateTime_Outcome','DateTime_Intake']][df['DateTime_Outcome'] < df['DateTime_Intake']]\n",
    "#print out the number of instances failing the test\n",
    "print(\"Number of rows failing the test: \", test_1.shape[0])\n",
    "#view the instances\n",
    "test_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - **Test 2:** Check if the *'DateTime_Intake'* for any animal is before their *'DateofBirth'*. This would imply that the animal entered the shelter before they were born which is impossible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows failing the test:  0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DateTime_Intake</th>\n",
       "      <th>DateofBirth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [DateTime_Intake, DateofBirth]\n",
       "Index: []"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a dataframe containing the instances where date of intake is less than date of birth\n",
    "test_2 = df[['DateTime_Intake','DateofBirth']][df['DateTime_Intake'] < df['DateofBirth']]\n",
    "#print out the number of instances failing the test\n",
    "print(\"Number of rows failing the test: \", test_2.shape[0])\n",
    "#view the instances\n",
    "test_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - **Test 3:** Check if age of outcome is less than age of intake for any animal. This would imply that the animal decreased in age in the shelter which is impossible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows failing the test:  0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AgeuponIntake</th>\n",
       "      <th>AgeuponOutcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [AgeuponIntake, AgeuponOutcome]\n",
       "Index: []"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a dataframe containing the instances where date of intake is less than date of birth\n",
    "test_3 = df[['AgeuponIntake','AgeuponOutcome']][df['AgeuponOutcome'] < df['AgeuponIntake']]\n",
    "#print out the number of instances failing the test\n",
    "print(\"Number of rows failing the test: \", test_3.shape[0])\n",
    "#view the instances\n",
    "test_3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are no instances with failing logical integrity tests. We can now process with further cleaning steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further cleaning steps\n",
    "\n",
    "Upon reivew of homework 1, we will now perform some additional cleaning steps which need to be carried out before proceeding with the steps involved in homework 2. These are: \n",
    "\n",
    "- Breed_Intake high cardinality\n",
    "- Colour_Intake high cardinality\n",
    "- SexuponIntake, SexuponOutcome \n",
    "- DateTime features\n",
    "- Length of stay\n",
    "- Remove AnimalID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Breed_Intake high cardinality**:\n",
    "Firstly, upon review of homework 1 I have decided that a more suitable grouping of the breed values could have been performed in order to overcome the issue of high cardinality. I have preserved the original values for *Breed_Intake* from homework1 in a feature called *BreedOriginal*. We will now perform a cleaning step on this oringinal feature in order to reduce the cardinality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Domestic Shorthair Mix       259\n",
       "Chihuahua Shorthair Mix       77\n",
       "Labrador Retriever Mix        56\n",
       "Pit Bull Mix                  52\n",
       "Domestic Shorthair            43\n",
       "Domestic Medium Hair Mix      26\n",
       "German Shepherd Mix           20\n",
       "Siamese Mix                   16\n",
       "Boxer Mix                     15\n",
       "Bat Mix                       15\n",
       "Pit Bull                      13\n",
       "Miniature Poodle Mix          11\n",
       "Bat                           11\n",
       "Domestic Longhair Mix         11\n",
       "Border Collie Mix             11\n",
       "Chihuahua Shorthair            9\n",
       "German Shepherd                8\n",
       "Australian Shepherd Mix        8\n",
       "Australian Cattle Dog Mix      8\n",
       "Rottweiler Mix                 7\n",
       "Name: BreedOriginal, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#look at the values \n",
    "df['BreedOriginal'].value_counts()[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is a lot of overlap between with some breeds having the word \"Mix\" appended to the name. Our first step will be to remove the word \"mix\" and thus combine similar breeds. For example, 'Pit Bull' will be combined with 'Pit Bull Mix'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['BreedOriginal'] = df['BreedOriginal'].str.replace(r' Mix', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As dogs and cats are the most frequent animal type, we will now analyse the different breeds for these. Firstly, we will look at the breeds of dogs and will perform some cleaning in order to reduce the number of different breeds. We will group any dog breeds with a value count of 10 or less into one value called \"Other Dog breeds\". To do this, we first need to create a feature which represents the value counts of the different breeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "animal_values_to_threshold_other = ['Dog', 'Cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for animal in animal_values_to_threshold_other:\n",
    "    value_counts = df['BreedOriginal'][df['AnimalType_Intake']==animal].value_counts()\n",
    "    df.loc[df['BreedOriginal'].isin(value_counts.index[value_counts < 10]), 'BreedOriginal'] = animal + \" Other\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now group all other animals into a value called \"Other Animal Breeds\". This is done because cats and dogs are the most frequent animal type and also because other animals are not divided into meaningful breeds in the same way that cats and dogs are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create variable all animals other than cats and dogs\n",
    "breeds_other_than_cat_or_dog = df[df['AnimalType_Intake']!='Cat'][df['AnimalType_Intake']!='Dog']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group them into the one value\n",
    "breeds_other_than_cat_or_dog['BreedOriginal'] = \"Other Animal Breeds\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update the df\n",
    "df.update(breeds_other_than_cat_or_dog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Domestic Shorthair       302\n",
       "Dog Other                269\n",
       "Chihuahua Shorthair       86\n",
       "Pit Bull                  65\n",
       "Labrador Retriever        63\n",
       "Other Animal Breeds       56\n",
       "Domestic Medium Hair      31\n",
       "German Shepherd           28\n",
       "Cat Other                 18\n",
       "Siamese                   18\n",
       "Boxer                     16\n",
       "Miniature Poodle          13\n",
       "Border Collie             13\n",
       "Domestic Longhair         12\n",
       "Australian Cattle Dog     10\n",
       "Name: BreedOriginal, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['BreedOriginal'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Color_Intake high cardinality**:\n",
    "Secondly, it was also found upon review of homework1 that the grouping I performed into 'Light' and 'Dark' could be improved. As with the breeds, we have gone back and created a temporary feature which preserves the orginial values within this feature. We will now use this temporary feature to perform some cleaning and reduce the high cardinality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new dataframe in which we will split the color feature into two features\n",
    "color_df = pd.DataFrame(columns=['firstcolor','secondcolor','ColorOriginal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the Color_Intake feature at the delimeter \"/\"\n",
    "color_df[['firstcolor','secondcolor']] = df['ColorOriginal'].str.split(\"/\",expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the unique values\n",
    "color_df['firstcolor'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conor = ['Tabby|Tortie|Torbie|Point|Calico|Merle|Tick', 'Yellow|Cream|Gold|Fawn|Buff']\n",
    "# bla = ['Mixed Pattern', 'Light Shades']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 1), ('b', 2), ('c', 3), ('d', 4)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for a,b in zip(conor, bla):\n",
    "#     print (a,b)\n",
    "\n",
    "\n",
    "# l1 = [\"a\",\"b\", \"c\",\"d\"]\n",
    "# l2 = [1,2,3,4,5]\n",
    "# list(zip(l1,l2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_df['firstcolor'][color_df.firstcolor.str.contains('Tabby|Tortie|Torbie|Point|Calico|Merle|Tick')] = 'Mixed Pattern'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_df['firstcolor'][color_df.firstcolor.str.contains('Yellow|Cream|Gold|Fawn|Buff')] = 'Light Shades'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_df['firstcolor'][color_df.firstcolor.str.contains('Brown|Chocolate|Sable|Tan')] = 'Brown Shades'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_df['firstcolor'][color_df.firstcolor.str.contains('Silver|Gray|Grey')] = 'Grey'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_df['firstcolor'][color_df.firstcolor.str.contains('Mixed Pattern|White|Black|Light Shades|Grey|Brown')==0] = 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ColorOriginal']=color_df['firstcolor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ColorOriginal'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sex features\n",
    "\n",
    "Upon review and further analysis of my homework1, I decided that the features *SexuponIntake* and *SexuponOutcome* should have been split into two separate features representing the gender and whether or not the animal was neutered upon intake and outcome. We have decided that it is best to perform this cleaning step here in order to most accurately choose the features to use for modelling.\n",
    "\n",
    "We will begin by extracting information about whether the animal was neutered or not. This information will be represented in two new features - *NeutereduponIntake* and *NeutereduponOutcome*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate through the 'SexuponIntake' and 'SexuponOutcome' and split into two new features\n",
    "for i in [\"Intake\", \"Outcome\"]:\n",
    "    temp_df = pd.DataFrame()\n",
    "    temp_df[['Neutered', 'Sex']] = df['Sexupon' + i].str.split(\" \", expand = True)\n",
    "    temp_df = temp_df.drop('Sex', axis = 1)\n",
    "    \n",
    "    temp_df.loc[temp_df['Neutered'].str.contains(\"Neutered\", case = False), \"Neutered\"]=\"True\"\n",
    "    temp_df.loc[temp_df['Neutered'].str.contains(\"Spayed\", case = False), \"Neutered\"]=\"True\"\n",
    "    temp_df.loc[temp_df['Neutered'].str.contains(\"Intact\", case = False), \"Neutered\"]=\"False\"\n",
    "    \n",
    "    df['Neuteredupon' + i] = temp_df['Neutered'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create two more features which represent the sex of the animal. In the code below we will also drop the two original features *SexuponIntake* and *SexuponOutcome*. No information will be lost as we have simply divided the information into four new features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split 'SexuponIntake' and 'SexuponOutcome' into Male and Female\n",
    "df['IntakeSex']='Male'\n",
    "df['OutcomeSex']='Male'\n",
    "\n",
    "df.loc[df['SexuponIntake'].str.contains(\"Female\"), \"IntakeSex\"] = \"Female\"\n",
    "df.loc[df['SexuponOutcome'].str.contains(\"Female\"), \"OutcomeSex\"] = \"Female\"\n",
    "\n",
    "#drop original features\n",
    "df = df.drop(['SexuponIntake', 'SexuponOutcome'], axis = 1)\n",
    "\n",
    "df['IntakeSex'] = df['IntakeSex'].astype('category')\n",
    "df['OutcomeSex'] = df['OutcomeSex'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that the sex of the animal at outcome is the same as intake. We will test this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['OutcomeSex'] != df['IntakeSex']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, there are no rows with different values for these two features. We can now remove *OutcomeSex*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('OutcomeSex', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Date features\n",
    "\n",
    "Next, for the datetime features *DateOfBirth*, *DateTime_Intake* and *DateTime_Outcome* some data integrity issues were dealt with in homework 1. Furthermore, in homework 1, some analysis was done of the month, day and hours of intake in a temporary dataframe, however, it was not deemed necessary at the time to split these features into categorical features representing the year, month and day. Before proceeding with predictive modelling we will now carry out this step. This step will make it easier to work with the high cardinality of the datetime features and it will convert them to a format which is easier to plots against the binary outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DateTime_Intake**\n",
    "We begin this step by converting the feature *DateTime_Intake* into year, month and day features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the year, month and day from 'DateTime_Intake'\n",
    "df['Intake_Year']=df['DateTime_Intake'].dt.year\n",
    "df['Intake_Month']=df['DateTime_Intake'].dt.month\n",
    "df['Intake_Day']=df['DateTime_Intake'].dt.day_name()\n",
    "df['Intake_Hour']=df['DateTime_Intake'].dt.hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When exploring the hour of intake in homework 1, we saw that animals arriving during the night had a higher likelihood of a negative outcome. As a result, since we are expecting these interesting results again, at this stage we will also create an additional feature which will groups the hours into four bins representing early morning, late morning, afternoon and evening/night."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bin the hours into four equal width groups\n",
    "df['Intake_Hour_bins']=pd.cut(df['Intake_Hour'],bins=[-1,6,12,18,24], labels=['EarlyMorning','LateMorning','Afternoon','Evening/Night'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DateTime_Outcome**\n",
    "We will now convert *DateTime_Outcome* into year, month day and hour features. In order to be consistent, we will also create an additional feature to bin the hour of outcome into groups of four."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the year, month and day from 'DateTime_Outcome'\n",
    "df['Outcome_Year']=df['DateTime_Outcome'].dt.year\n",
    "df['Outcome_Month']=df['DateTime_Outcome'].dt.month\n",
    "df['Outcome_Day']=df['DateTime_Outcome'].dt.day_name()\n",
    "df['Outcome_Hour']=df['DateTime_Outcome'].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bin the hours into four equal width groups\n",
    "# df['Outcome_Hour_bins']=pd.cut(df['Outcome_Hour'],bins=[-1,6,12,18,24], labels=['Early_Morning','Late_Morning','Afternoon','Evening/Night'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DateOfBirth**\n",
    "We will now convert *DateofBirth* into year, month and day features. Hour is not included in this feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the year, month and day from 'DateTime_Outcome'\n",
    "df['Birth_Year']=df['DateofBirth'].dt.year\n",
    "df['Birth_Month']=df['DateofBirth'].dt.month\n",
    "df['Birth_Day']=df['DateofBirth'].dt.day_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now created new features from the datetime features. These new features have a much lower cardinality and are in a format that is much more useful for predictive modelling. We will now convert these new features to type 'category' and will ensure all features are of the correct type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert all int64 to categories\n",
    "int_columns = df.select_dtypes(['int64']).columns\n",
    "for column in int_columns:\n",
    "    df[column] = df[column].astype('category')\n",
    "\n",
    "#convert all objects to categories\n",
    "object_columns = df.select_dtypes(['object']).columns\n",
    "for column in object_columns:\n",
    "    df[column] = df[column].astype('category')\n",
    "    \n",
    "#convert all float64 to int64\n",
    "float_columns = df.select_dtypes(['float64']).columns\n",
    "for column in float_columns:\n",
    "    df[column] = df[column].astype('int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Length of Stay:**\n",
    "Finally, in homework 1 we did indeed explore the length of stay of animals in the shelter when carrying out data integrity tests. However, we did not make a new feature to represent this information and investigate its relationship with the binary outcome. Upon review of homework1, we now feel this is necessary. As a result, we will create this feature by subtracting the date of intake from the date of outcome. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new feature length of stay\n",
    "df['LengthOfStay'] = (df['DateTime_Outcome'] - df['DateTime_Intake']).dt.days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now drop the original features *DateTime_Intake*, *DateTime_Outcome* and *DateofBirth* as we have extracted all information from them into the new features and no information will be lost. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the features 'DateTime_Intake', 'DateTime_Outcome' and 'DateofBirth'\n",
    "df = df.drop('DateofBirth', 1)\n",
    "df = df.drop('DateTime_Intake', 1)\n",
    "df = df.drop('DateTime_Outcome', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step in our further cleaning is to drop the feature 'AnimalID'as this is only needed for reference purposes and is not helpful in predictive modelling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop 'AnimalID'\n",
    "df = df.drop('AnimalID', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing values were dealt with in homework1. However, we will now check to ensure that there are currently no missing values in our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are no missing values. We will now check the datatypes again before proceeding with plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now setup our categorical and continuous features. These will be needed for plotting and for the creation of dummies later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = df.select_dtypes(['category']).columns\n",
    "continuous_features = ['AgeuponIntake', 'AgeuponOutcome', 'LengthOfStay']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.1) Randomly shuffle the rows of your dataset and split the dataset into two datasets: 70% training and 30% test. Keep the test set aside. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding with splitting the dataset we weill first take a look at the ratio of different levels of the target categorical feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['binary_outcome'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn train_test_split randomly shuffles and samples the dataset as per the test size given as a parameter. This random sampling is often a good choice in order to avoid introducing bias. However, as explained in section 3.6.3 of the course text book 'Fundamentals of Machine Learning for Predictive Data Analytics', since only small proportion of the instances in our dataset are of class 1, there is a chance if we use random sampling these instances will be omitted or underrepresented. \n",
    "\n",
    "For this reason, we will use stratified sampling instead of random sampling. This will ensure that the relative frequencies of the two levels of our target feature *binary_outcome* are maintained in the sampled dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The dataset will now be split into two separate datasets - 70% training and 30% test using stratified sampling**\n",
    "- We set the target feature \"y\" to be \"binary_outcome\"\n",
    "- We feature \"x\" to be all remaining features in the dataset. The feature \"binary_outcome\" will be excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = df.binary_outcome\n",
    "y = pd.DataFrame(df[\"binary_outcome\"])\n",
    "X = df.drop([\"binary_outcome\"],1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now split the dataset. The parameter 'test-size' will determine the size of the training and test datasets. We will set this to 0.3 in order to split into 70% training and 30% test. The parameter 'random_state' sets a seed to the train_test_split random generator. We will set this parameter to 1 to ensure that the train/test split is the same each time this code is executed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code was provided in lab 7.\n",
    "# Split the dataset into two datasets: 70% training and 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=1, stratify=y)\n",
    "\n",
    "print(\"original range is: \",df.shape[0])\n",
    "print(\"training range (70%):\\t rows 0 to\", round(X_train.shape[0]))\n",
    "print(\"test range (30%): \\t rows\", round(X_train.shape[0]), \"to\", round(X_train.shape[0]) + X_test.shape[0])\n",
    "\n",
    "print(\"\\nTarget feature binary_outcome total dataset counts:\\n\\n\", y['binary_outcome'].value_counts())\n",
    "print(\"\\nTraining data binary_outcome counts:\\n\", y_train['binary_outcome'].value_counts())\n",
    "print(\"\\nTest data binary_outcome counts:\\n\", y_test['binary_outcome'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the output printed above that class 1 accounts for 8% of the instances in the full dataset and in the training and test splits. Thus, the train/test samples have a distribution representative of the full sample for class 0 and class 1 of the target feature *binary_outcome*. We are now ready to plot the features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1.2) On the training set - For each continuous feature, plot its interaction with the target feature. Discuss what you observe from these plots. Choose a subset of continuous features you find promising (if any)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- All plots will be generated using on the training subset created above - X_train, y_train. The testing subset will be left aside.\n",
    "- We will use the lists of categorical features, continuous features and date_time features created above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 Plot correlations between the continous features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix using code found on https://stanford.edu/~mwaskom/software/seaborn/examples/many_pairwise_correlations.html\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "# Calculate correlation of all pairs of continuous features\n",
    "corr = X_train[continuous_features].corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(20, 20))\n",
    "\n",
    "# Generate a custom colormap - blue and red\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, annot=True, mask=mask, cmap=cmap, vmax=1, vmin=-1,\n",
    "            square=True, xticklabels=True, yticklabels=True,\n",
    "            linewidths=.5, cbar_kws={\"shrink\": .5}, ax=ax)\n",
    "plt.yticks(rotation = 0)\n",
    "plt.xticks(rotation = 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the following in the above correlation matrix: \n",
    "-  *AgeUponIntake* and *AgeUponOutcome* have an extremely high correlation of 1. This is an expected and indicates that only one of these features would need to be kept. \n",
    "-  Both *AgeUponIntake* and *AgeUponOutcome* are very weakly correlated with length of stay, with values of 0.059 and 0.15, respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 Plot correlation between continous features and target feature. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will now loop over both continuous features and plot them against the target *binary_outcome*. \n",
    "- We will discuss our findings from these plots and choose a subset (if any) of the feature(s) which seem promising. \n",
    "- As we left the target feature as type int64 earlier, we do not need to convert it now for these scatter plots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict to hold correlation values \n",
    "corr_dict = {}\n",
    "\n",
    "# plot pairwise interaction between all continuous features and target\n",
    "for column in X_train[continuous_features]:\n",
    "    # create temp df to merge column and target\n",
    "    df_temp = pd.concat([X_train[column], y_train], axis=1)\n",
    "    # store correlation in variable\n",
    "    correlation = df_temp[[column, \"binary_outcome\"]].corr().values[0,1]\n",
    "    # plot the column and tartget feature\n",
    "    df_temp.plot(kind='scatter', x=column, y=\"binary_outcome\", label=\"%.3f\" % correlation)\n",
    "    # add correlation to dict\n",
    "    corr_dict[column] = correlation\n",
    "\n",
    "# dataframe holding sorted correlation values to aid in interpreting results\n",
    "corr_df = pd.DataFrame.from_dict(corr_dict, orient='index', columns=['binary_outcome']).sort_values('binary_outcome', ascending=False)\n",
    "corr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 Interpretation of the results\n",
    "\n",
    "- We can see that there is no clearly defined correlation between either feature and the target feature.\n",
    "- The result is surprising as it was expected that the age of an animal would have an effect on its outcome. \n",
    "- However, this result is perhaps explained by the large range of ages and the variety of animals which the ages represent. \n",
    "- The sorted correlation dataframes above the plots give an even clearer indication of the results. We can see that the correlation between *AgeUponIntake* and the target feature is 0.059 while the correlation between *AgeUponOutcome* and the target feature is 0.054.\n",
    "- It must be noted however, that as described in section 3.6.1 of the FMLPD course textbook, continuous features that cover very different ranges should be normalised. As we are dealing with the ages of many different species of animals, normalisation would cause these two continous features to fall within a specified range while maintaining the relative differences between the values for the feature. In other words, adding a normalisation step would better represent the different between an old dog and an old racoon, for example. \n",
    "- At this stage in the assignment however, we are choosing the leave the ages at their absolute values. Later in the assignment we will have the option to implement normalisation and make a comparision.\n",
    "- The correlation between *lengthOfStay* and the outcome is -0.052. This feature will be dropped as these results show that it is likely not an indicator of the binary outcome. \n",
    "- As a result, at this stage we will drop these two features as they do not look like good predictors of the target outcome. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO Investigating whether to normalise ages........."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict to hold correlation values \n",
    "# corr_dict = {}\n",
    "\n",
    "# # plot pairwise interaction between all continuous features and target\n",
    "# for column in X_train[X_train[\"AnimalType_Intake\"]==\"Cat\"][continuous_features]:\n",
    "#     # create temp df to merge column and target\n",
    "#     df_temp = pd.concat([X_train[X_train[\"AnimalType_Intake\"]==\"Cat\"][column], y_train], axis=1)\n",
    "#     # store correlation in variable\n",
    "#     correlation = df_temp[[column, \"binary_outcome\"]].corr().values[0,1]\n",
    "#     # plot the column and tartget feature\n",
    "#     df_temp.plot(kind='scatter', x=column, y=\"binary_outcome\", label=\"%.3f\" % correlation)\n",
    "#     # add correlation to dict\n",
    "#     corr_dict[column] = correlation\n",
    "\n",
    "# # dataframe holding sorted correlation values to aid in interpreting results\n",
    "# corr_df = pd.DataFrame.from_dict(corr_dict, orient='index', columns=['binary_outcome']).sort_values('binary_outcome', ascending=False)\n",
    "# corr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now we will label the two continuous features as ```low_correlation_continuous_features``` and will remove them after analysing the categorical features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_correlation_continuous_features = ['AgeuponIntake', 'AgeuponOutcome', 'LengthOfStay']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3 Plot interaction between categorical features and target feature.\n",
    "\n",
    "- We will now plot a pairwise interaction between each categorical feature and the target feature. \n",
    "- We will discuss which features seem to be better at predicting the target feature and will choose a subset of features we find promising.\n",
    "- As with the plots of continuous features above, only the training dataset will be used. \n",
    "- To create these plots we will work with the variable *categorical_features* created above. \n",
    "- We will begin by converting the target feature to a categorical type as this is required for these plots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the comparison we will convert the target back to categorical\n",
    "y_train = y_train.astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# X_train[categorical_features].columns\n",
    "\n",
    "# we will create temp dataframe for these plots, mergeing X_train and y_train\n",
    "df_temp= pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "\n",
    "# for each categorical feature create a stacked bar plot\n",
    "for categorical_feature in categorical_features:\n",
    "    # Using code from the module lab\n",
    "\n",
    "    values = pd.unique(df_temp[categorical_feature].ravel())\n",
    "\n",
    "    # create percent column and initalise to 0\n",
    "    df_temp['percent'] = 0\n",
    "\n",
    "    #print header\n",
    "    print(\"\\n\", categorical_feature)\n",
    "    print(\"Index \\t Count\")\n",
    "\n",
    "    # loop through each value in the feature col1\n",
    "    for i in values:\n",
    "\n",
    "        count = df_temp[df_temp[categorical_feature] == i].count()['binary_outcome']\n",
    "        count_percentage = (1 / count) * 100\n",
    "\n",
    "        # print out index vs count\n",
    "        print(i, \"\\t\", count)\n",
    "\n",
    "        index_list = df_temp[df_temp[categorical_feature] == i].index.tolist()\n",
    "        for ind in index_list:\n",
    "            df_temp.loc[ind, 'percent'] = count_percentage\n",
    "\n",
    "    group = df_temp[['percent',categorical_feature,'binary_outcome']].groupby([categorical_feature,'binary_outcome']).sum()\n",
    "\n",
    "    #plot the results\n",
    "    my_plot = group.unstack().plot(kind='bar', stacked=True, title=f\"Binary Outcome vs {categorical_feature}\", figsize=(15,7), grid=True)\n",
    "\n",
    "    # add legend\n",
    "    red_patch = mpatches.Patch(color='orange', label='Class 1')\n",
    "    blue_patch = mpatches.Patch(color='blue', label='Class 0')\n",
    "    my_plot.legend(handles=[red_patch, blue_patch], frameon = True)\n",
    "\n",
    "    # add gridlines to make the plots easier to visualise\n",
    "    plt.grid(b=True, which='major', color='#666666', linestyle='-')\n",
    "    plt.minorticks_on()\n",
    "    \n",
    "    plt.grid(b=True, which='minor', color='#999999', linestyle='-', alpha=0.2)\n",
    "\n",
    "    my_plot.set_xlabel(categorical_feature)\n",
    "    my_plot.set_ylabel(\"%risk\")\n",
    "    my_plot.set_ylim([0,100])\n",
    "    \n",
    "# drop 'percent' that was used only for stacked bar plot\n",
    "df_temp = df_temp.drop(\"percent\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.4 Interpretation of the results\n",
    "\n",
    "As we have a lot of categorical features, we will only give a brief discussion of each. \n",
    "\n",
    "- **Name_Provided**: During data cleaining in homework1, we converted the values of this feature to represent whether an animal has a name of not. We can see that animals with no name are 20% more likely to have a negative outcome. This feature will be kept. \n",
    "\n",
    "- **FoundLocation**: We can see that if an animal is found in Austin, Del Valle, Pflugerville or Travis, they are 5-10% more likely to have a negative outcome than if found at the other locations. In fact, we see no negaitve outcome at any other locations. Upon examination of the counts however, we can see that this is most likely due to the cities mentioned having much larger value counts. This feature will be dropped.\n",
    "\n",
    "- **Intake_Type**: We can see here that intake type has some interesting findings. As expected, all euthenasia requests result in a negative outcome. Interestingly, all animals with intake type *Wildlife* also have a negative outcome. This feature will be kept. \n",
    "\n",
    "- **IntakeCondition**: Here we can see that if an animal is sick or injured, they are 40-50% more likely to have a negative outcome. Although when looking at the absolute values, we can see that the sick or injured animals only make up approximately 10% of the total, the graphs show that this feature still seems like a strong predictor. This was accounted for in homework1 with the new feature *SickOrInjured*. \n",
    "\n",
    "- **AnimalType_Intake**: We see in this plot that animals of type *Bat* or *Other* are 100% and 65% more likely to have a negative outcome, respecitvely, than animals of type *Bird, Cat* or *Dog*. On further inspection of the value counts however, we can see that there are only 5 birds. Thus, it seems that animals of type *Cat* and *Dog* are most likely to have a positive outcome. This is accounted for in the new feature *CatOrDog* made in homework1. \n",
    "\n",
    "\n",
    "## TODO finalise sex features\n",
    "- **IntakeSex: We can see that males are approximately 10% more likely to be in class 1 than females. This feature will be kept.** \n",
    "\n",
    "- **NeutereduponIntake, NeutereduponOutcome We can see that there is a strong correlation between the value unknown and class 1. We can also see a small correlation between animals not being neutered upon outcome and class 1. These two features are providing very similar information, with *NeutereduponOutcome* providing additional information. As a result, we will drop *NeutereduponIntake* and keep NeutereduponOutcome.**\n",
    "\n",
    "- **BreedOriginal**: We can see that breeds of other animals are more likely to have a negative outcome. This is unsurprising after analysing the *AnimalType_Intake* plot and seeing that cats and dogs are most likely to have a positive outcome. We can also see some small correlations between other breeds and the binary outcome. For example, *Pit Bull* and *Poodle* dogs both have are more likely to be in class 1 than other breeds. Similarly, we can see that *Siamese* cats are more likely to have a negative outcome than other cat breeds. As a result, this feature will be kept. \n",
    "\n",
    "- **ColorOriginal**: We can see some small correlations here. *Grey/Silver* has the worst outcome while *White* and *LightColour* animals are most likely to be class 0. This feature will be kept. \n",
    "\n",
    "- **Breed_Intake, Color_Intake**: Neither of these show any correlation with the outcome and shall both be dropped. \n",
    "\n",
    "**new features created in homework1**:\n",
    "- **SexKnown**: Upon further analysis since creating this feature in homework1, we have found that this feature is not helpful and will be dropped. \n",
    "- **CatOrDog, SickOrInjured**: Both of these new features show a strong correlation with the binary outcome and will both be kept. \n",
    "- **AgeIntake_bins**: We can see that there is no clear pattern between the age of intake bins and the binary outcome. It was expected that older animals would be more likely to have a negative outcome due to both a higher likelihood of death and a lower likelihood of being adopted. However, we can see that the greatest likelihood of a negative outcome is actually seen in the bin representing an age of 365-427 days. The likelihood of a negative outcome then decreases as the ages increase. This feature will be dropped. \n",
    "\n",
    "**Year, Month, Day, and hour features**:\n",
    "\n",
    "Categorical features were created for each of the datetime features above. As there are too many to discuss, we will discuss only the ones for which we see a correlation.\n",
    "- **Intake_Hour** is interesting. It seems that there is a much higher likelihood of a negative outcome if the animal is taken into the shelter during the night, with intake at 2am having a 100% likelihood of a negative outcome. On further investigation we can see that the number of animals being dropped in at these times is very low compared to the daylight hours. However, after doing some research into the Austin Animal center, I have learned that the opening hours are 11am-7pm. This information increases the relevance of the results seen during the night, as it is expected that the animals will not have as good an outcome if found or left at the shelter outside opening hours.\n",
    "- **Intake_Hour_bins:** As expected, the information in *Intake_Hour* is well represented in this feature. We can see that an animal that enters the shelter in the early morning (0-6am) has a 30% likelihood of being in class 1 and an animal entering the shelter in the evening/night (6pm-midnight) has a 20% likelihood of being in class 1. These findings are in comparison to the other two values which show a 5-10% likelihood of the animal being in class 1. This feature can be kept and **Intake_Hour** can be dropped.\n",
    "- **Outcome_Hour**: We can see that between 7-10am, animals have a higher likelihood of being in class 1. This is most likely due to animals who have died overnight being found in the morning once the shelter opens. This feature will be kept.\n",
    "- **Outcome_Hour_bins:** As expected, we can see that animals with a time of outcome in the late morning have a higher likelihood of a negative outcome. This feature will be kept and **Outcome_Hour** will be dropped. \n",
    "- **All other datetime related features**: For all other features created from the datetime features, no clear correlation was found upon analysis of the plots and the value counts. These will be dropped. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.5 Summary and drop features\n",
    "\n",
    "From continuous vs target feature plots (1.2.1) we saw that none of the continious features showed a high correlation with the target feature. These features will be dropped. \n",
    " \n",
    "From categorical vs target feature (1.2.3) we saw that the following features did not show a clear relationship with the target feature and can be removed for now:\n",
    "\n",
    "    - FoundLocation\n",
    "    - Breed_Intake\n",
    "    - ColorIntake\n",
    "    - Intake_Year\n",
    "    - Intake_Month\n",
    "    - Intake_Day\n",
    "    - Intake_Hour\n",
    "    - Outcome_Year\n",
    "    - Outcome_Month\n",
    "    - Outcome_day\n",
    "    - Outcome_Hour\n",
    "    - Birth_Year\n",
    "    - Birth_Month\n",
    "    - Birth_Day\n",
    "    - AnimalType_Intake\n",
    "    - IntakeCondition\n",
    "    - SexKnown\n",
    "    - NeutereduponOutcome\n",
    "    - AgeIntake_Bins\n",
    "\n",
    "\n",
    "We will leave all other categorical features in for now. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now create a list of the low information categorical features listed above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_information_gain_categorical_features = ['FoundLocation',\n",
    "                                             'Breed_Intake',\n",
    "                                             'Color_Intake',\n",
    "                                             'Intake_Year',\n",
    "                                             'Intake_Month',\n",
    "                                             'Intake_Day',\n",
    "                                             'Intake_Hour',\n",
    "                                             'Outcome_Year',\n",
    "                                             'Outcome_Month',\n",
    "                                             'Outcome_Day',\n",
    "                                             'Birth_Year',\n",
    "                                             'Birth_Month',\n",
    "                                             'Birth_Day',\n",
    "                                             'SexKnown',\n",
    "                                             'AnimalType_Intake',\n",
    "                                             'IntakeCondition',\n",
    "                                             'AgeIntake_bins',\n",
    "                                             'NeutereduponIntake']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now drop the low impact categorical features listed above. \n",
    "\n",
    "- Before dropping we will duplicate the original dataframe *df* and label the duplicate as *df_version1*\n",
    "- We will drop features from the duplicate *df_version1* only.\n",
    "- This will allow us to revert to the original dataset later if we decide to compare the performance of the reduced dataset *df_version1* with the original dataset *df*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_value_features = list(set(low_correlation_continuous_features + low_information_gain_categorical_features))\n",
    "# before dropping make copy of original\n",
    "df_version1 = df.copy()\n",
    "# drop low value features\n",
    "df_version1.drop(low_value_features, 1, inplace=True)\n",
    "print('\\nRemaining columns:', df_version1.columns)\n",
    "print('\\nNew shape:', df_version1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Prepare the dataset for modelling\n",
    "\n",
    "Now we have picked our descriptive features for the whole dataset, we will now carry out a number of additional steps in order to prepare the dataset for modelling: \n",
    "- Convert the categorical variables into dummies variables so that they can be used for modelling.\n",
    "- Remove the redundant dummies which contain no additional information.\n",
    "- Set up up the train test split again based on the dataset with the dummies included.\n",
    "\n",
    "We will now go through these steps in more detail. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, as described in section 7.4.3 of the course textbook 'Fundamentals of Machine Learning for Predictive Data Analytics', the most common approach to handling categorical features in regression models is to use a transformation that converts a single categorical feature into a number of continuous features that can encode the levels of the categorical feature. The levels of the categorical feature are encoded by setting the value of the new continuous feature which corresponds to the level of the categorical feature to 1, and the other continious features to 0. This is known as *dummy encoding* and is the approoach we will now take. \n",
    "\n",
    "The downside of this approach however, is that it creates a lot of new features in the dataset. In other words, as we will see in the linear regression steps below, it creates many new weights for which optimal values must be found. For this reason we carry out the second step above - remove the redundant dummies which contain no additional information. For example, we can assume that a zero in all new features implies that the original cateogrical feature had the first value. We will add the parameter *drop_first = True* in the code below in order to execute this step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up dummies features\n",
    "df_version1 = pd.get_dummies(df_version1, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at the dummies \n",
    "df_version1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The features are:**\n",
    "\n",
    "As we are not keeping any of our continuous features for now, our features are comprised only of the *uint8* types generated when making dummies above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put all features together into a variable\n",
    "features = df_version1.select_dtypes(include=['uint8']).columns.tolist()\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/test split\n",
    "- We will now split the dataset into train/test subsets using the dummy values created above.\n",
    "- As explained in section 1.1 above, we will use stratified sampling.\n",
    "- The target *binary_outcome* is stored in dataframe \"y\".\n",
    "- The remaining features are stored in dataframe \"X\".\n",
    "- Both are split into training and test subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up x and y \n",
    "y = df_version1['binary_outcome']\n",
    "X = df_version1.drop([\"binary_outcome\"],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into two datasets: 70% training and 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=1, stratify=y)\n",
    "\n",
    "print(\"original range is: \",df_version1.shape[0])\n",
    "print(\"training range (70%):\\t rows 0 to\", round(X_train.shape[0]))\n",
    "print(\"test range (30%): \\t rows\", round(X_train.shape[0]), \"to\", round(X_train.shape[0]) + X_test.shape[0])\n",
    "\n",
    "print(\"\\nTarget feature binary_outcome total dataset counts:\\n\\n\", y.value_counts())\n",
    "print(\"\\nTraining data binary_outcome counts:\\n\", y_train.value_counts())\n",
    "print(\"\\nTest data binary_outcome counts:\\n\", y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, as we have used stratified sampling, we can see that the ratio of positive to negative outcome is equal in all splits. \n",
    "\n",
    "We will now look at the first five lines of the training and test subsets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nDescriptive features in X_train:\\n\", X_train.head(5))\n",
    "print(\"\\nTarget feature in y_train:\\n\", y_train.head(5))\n",
    "print(\"\\nDescriptive features in X_test:\\n\", X_test.head(5))\n",
    "print(\"\\nTarget feature in y_test:\\n\", y_test.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reset the indexes of the training and test splits\n",
    "- We can see from the output above that the indexes are no longer consecutive for any of the dataframes. \n",
    "- We will fix this now so that we are able to merge the dataframes later in the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to reset the index to allow contatenation with predicted values otherwise not joining on same index...\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "y_test.reset_index(drop=True, inplace=True)\n",
    "X_train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to model the datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Linear Regression\n",
    "\n",
    "#### 2.1) On the training set, train a linear regression model to predict the target feature, using only the descriptive features selected in exercise (1) above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train aka fit, a model using all continuous and categorical features.\n",
    "multiple_linreg = LinearRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2.2) Print the coefficients learned by the model and discuss their role in the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print out the weights that have been learned for each feature.\n",
    "print(\"\\nFeatures are: \\n\", X_train.columns)\n",
    "print(\"\\nCoeficients are: \\n\", multiple_linreg.coef_)\n",
    "print(\"\\nIntercept is: \\n\", multiple_linreg.intercept_)\n",
    "print(\"\\nFeatures and coeficients: \\n\", list(zip(X_train.columns, multiple_linreg.coef_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion: \n",
    "\n",
    "Before discussing the weights printout out above, we will first introduce linear regression. \n",
    "\n",
    "- Linear regression is a supervised machine learning method which is used to make predictions based on a linear relationship between the target (dependent variable) and any number of independent variables (also known as predictors).\n",
    "- It finds the line of best fit that describes the relationship between the target and predictors. \n",
    "- The linear regression formula takes the following form:\n",
    "    - $target\\_feature = w_0 + w_1 * feature_1 + w_2*feature_2 + ...+ w_n*feature_n $\n",
    "    \n",
    "- In the formula, w_0, w_1 etc are known as the **model coefficients**. \n",
    "- To create the predictive model using linear regression, these coefficients must be \"learned\". \n",
    "- As seen in the image below, this is usually done using the **least squares criterion**. In other words, we find the line of best fit that describes the relationship between the target and predictors which minimises the **sum of squared residuals** (also known as sum of squared errors).\n",
    "<br><img src=\"linear_regression.png\" style=\"width: 300px;\"/> <br>\n",
    "Source: https://github.com/justmarkham/DAT4/blob/master/notebooks/08_linear_regression.ipynb\n",
    "\n",
    "- We will now understand the image above: \n",
    "    - The **black** dots are the observed values of x and y.\n",
    "    - The **blue line** is the least squares line. \n",
    "    - The **red lines** are the residuals. These are the distances between the observed values and the least squares line. It is these residuals that we try to minimise in linear regression.\n",
    "    \n",
    "- Linear regression is a regression task, which means that it predicts a numeric target feature. Thus, the output from the formula above is a continuous value that can be less than 0 and higher than 1.\n",
    "- It is important to note one of the downfalls of linear regression is that it is sensitive to outliers. However, this does not apply to our categorical features. \n",
    "\n",
    "**Discussing the coefficients learned by the model:**\n",
    "- We can see above that the intercept is 1.03 (this is w_0 in the formula above). This is the value of y when x is zero. In other words it is where the model intercepts the x axis. \n",
    "- If all other coefficients were zero, the model result would be 1.03.\n",
    "- We can see the coefficients for each feature above. They are zipped together in a single list for ease of visualisation and interpretation.\n",
    "- All of our features are categorical features which are dummy coded. Thus, care needs to be taken when interpreting the coefficients. The first value of each feature (which we have dropped) is the reference and each dummy is compared to the reference.\n",
    "- For example, we see that Name_Provided_Yes decreases by 0.086%. This makes sense as we saw in our barplot above that animals with no name are more likely to be in class 1.\n",
    "\n",
    "**Classification problem**\n",
    "- We are trying to solve a classification problem and thus it is important to note that the output from a linear regression model is not suited to this type of problem.\n",
    "- The output we get from linear regression is a numeric value and is not a probability. As a result, we will need to perform a thresholding step in order to convert the output into a binary classification. \n",
    "- The threshold will be as follows: \n",
    "    - Any values >= 0.5 will be cast to 1\n",
    "    - Any values <0.5 will be cast to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Printing 10 predicted target features and evaluate the prediction\n",
    "- Here we will print the predicted target feature value for the first 10 training examples. \n",
    "- As discussed above, we will now threshold the predicted target feature value given by the linear regression model at 0.5. This will give us the predicted class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the prediction and threshold the value. If >= 0.5 its true\n",
    "multiple_linreg_predictions_train = (multiple_linreg.predict(X_train) >= 0.5) * 1.0\n",
    "\n",
    "print(\"\\nPredictions with multiple linear regression: \\n\")\n",
    "actual_vs_predicted_multiplelinreg = pd.concat([y_train, pd.DataFrame(multiple_linreg_predictions_train, columns=['Predicted'])], axis=1)\n",
    "print(actual_vs_predicted_multiplelinreg.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 Evaluation metrics based on training data\n",
    "- We will now print the classification evaluation measures computed on the training set. These are:\n",
    "    - Accuracy\n",
    "    - Precision\n",
    "    - Confusion Matrix\n",
    "    - Recall\n",
    "    - F1\n",
    "    \n",
    "## TODO explain using auc roc    \n",
    "\n",
    "\n",
    "- We will then discuss our findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print evaluation measures\n",
    "print(\"==================== Train Data =======================\")\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_train, multiple_linreg_predictions_train))\n",
    "print(\"Confusion matrix: \\n\", metrics.confusion_matrix(y_train, multiple_linreg_predictions_train))\n",
    "print(\"Classification report:\\n \", metrics.classification_report(y_train, multiple_linreg_predictions_train))\n",
    "auc_linear_train = roc_auc_score(y_train, multiple_linreg_predictions_train)\n",
    "print(\"Area under the curve: \", auc_linear_train)\n",
    "print(\"======================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation of results\n",
    "\n",
    "We will now explain what each of the measures above means and will discuss our results. \n",
    "\n",
    "- **Accuracy** tell us how often the model is correct. \n",
    "    - We have an accuracy of 95%\n",
    "- **Confusion Matrix** tells us the following:\n",
    "    - [0][0] TRUE NEGATIVE - The number predicted 0 that was actually 0\n",
    "    - [0][1] FALSE POSITIVE - The number predicted 1 that is actually 0\n",
    "    - [1][0] FALSE NEGATIVE - The number predicted 0 that is actually 1\n",
    "    - [1][1] TRUE POSITIVE - The number predicted 1 that is actually 1\n",
    "    - We can see that we have 32 false negatives. This is quite a high value. \n",
    "- **Precision** tells us the following:\n",
    "    - What percentage of the predicted positive values are actually positive.\n",
    "    - How good the model is at predicting the positive class.\n",
    "    - It is the number of values which were correctly predicted positive over the total number of positive values predicted.\n",
    "    - Our precision negative is 0.95\n",
    "    - Our precision positive is 0.96\n",
    "- **Recall** tells us what % of the positive values we succesfully predicted.\n",
    "    - It is the number of values which were correctly predicted positive over the total number of positive values. \n",
    "    - Our recall negative is 1.00.\n",
    "    - Our recall positive is 0.43. This low result is due to the large number of false negatives mentioned above. \n",
    "- **F1 Score** is a weighted average of precision and recall. \n",
    "    - Our F1 score negative is 0.97.\n",
    "    - Our F1 score positive is 0.59.\n",
    "    \n",
    "**Summary**\n",
    "- Some of the results are quite good. \n",
    "- However, the positive recall is quite low. We can see that the model is better at predicting the negative class. \n",
    "- This makes sense considering that the majority class is the negative class. As a result, the model has more negative data to learn from. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6 Evaluate the model using the hold-out (30% examples) test set\n",
    "- The results from the test data will be compared the results from the training data.\n",
    "- In addition they will be compared to the results from a cross-validated model (i.e. a new model trained and evaluated using cross-validation on the full dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the prediction and threshold the value. If >= 0.5 its true\n",
    "multiple_linreg_predictions_test = (multiple_linreg.predict(X_test) >= 0.5) * 1.0\n",
    "\n",
    "print(\"\\nPredictions with multiple linear regression: \\n\")\n",
    "actual_vs_predicted_multiplelinreg = pd.concat([y_test, pd.DataFrame(multiple_linreg_predictions_test, columns=['Predicted'])], axis=1)\n",
    "print(actual_vs_predicted_multiplelinreg.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation metrics based on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print evaluation metrics.\n",
    "print(\"==================== Test Data =======================\")\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_test, multiple_linreg_predictions_test))\n",
    "print(\"Confusion matrix: \\n\", metrics.confusion_matrix(y_test, multiple_linreg_predictions_test))\n",
    "print(\"Classification report - Test data:\\n \", metrics.classification_report(y_test, multiple_linreg_predictions_test))\n",
    "auc_linear_test = roc_auc_score(y_test, multiple_linreg_predictions_test)\n",
    "print(\"Area under the curve: \", auc_linear_test)\n",
    "print(\"\\n==================== Train Data ======================\")\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_train, multiple_linreg_predictions_train))\n",
    "print(\"Confusion matrix: \\n\", metrics.confusion_matrix(y_train, multiple_linreg_predictions_train))\n",
    "print(\"\\nClassification report: - Training data\\n \", metrics.classification_report(y_train, multiple_linreg_predictions_train))\n",
    "auc_linear_train = roc_auc_score(y_train, multiple_linreg_predictions_train)\n",
    "print(\"Area under the curve: \", auc_linear_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation of test results and comparison with training set\n",
    "\n",
    "We will now discuss the results for the test data and compare it to the training data.\n",
    "- **Accuracy:** \n",
    "    - The accuracy of both the training and test data are 95%.\n",
    "- **Precision:** \n",
    "    - The precision score for predicting the negative class is also 95%.\n",
    "    - The precision score for predicting the positive class has decreased from 96% to 92%.\n",
    "- **Recall:**\n",
    "    - The recall score for predicting the negative class is the same at 100%\n",
    "    - The recall score for predicting the positive class has increased from 43% t0 46%.\n",
    "- **F1:**\n",
    "    - The f1 score for predicting the negative class has increased from 97% to 98%.\n",
    "    - The f1 score for predicting the positive class has increased from 59% to 61%.\n",
    "\n",
    "**Summary**\n",
    "- We are pleased with these results as we are now testing the predictive model on data that has not been seen before. The recall for the positive class has increased. This is a good sign that the model is generalising to new data. \n",
    "- We will now explore this even further using cross validation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.7 Cross-validation\n",
    "\n",
    "- We will now perform cross-validation on the linear regression model. \n",
    "- Using cross validation we do repeated train/test splits and average the error on the test datasets. Thus, it can result in a less biased result than a simple train/test split. \n",
    "- To perform cross-validation we will randomly split the dataset into 10 equal partitions. We will then use one partition as the test set and the union of all other partitions as the training set. We then evaluate the results of the test set. We will then repeat these steps using a different partition as the test set for each iteration. Finally, we will take the average of all results. \n",
    "\n",
    "We will begin by creating a function to perform cross-validation as this is not provided by Scikit-Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this code is from the sample solution provided\n",
    "def cross_val_LinReg(X, y, threshold, cv=3, scoring='accuracy', resampled = None):\n",
    "    \"\"\"Functions to carry out cross validation on the linear regression model\n",
    "    Default number of validations is 3. The randon state will be updated \n",
    "    at each iteration to allow our results to be repeated\"\"\"\n",
    "    \n",
    "    # store results\n",
    "    results = []\n",
    "    # evaluate cv times and append to results\n",
    "    for i in range(cv):\n",
    "        # set up train test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=i , test_size=0.3)\n",
    "        # generate model\n",
    "        if resampled == \"resampled\":\n",
    "            X_train_resampled, y_train_resampled = over_sample_minority(X_train, y_train)\n",
    "            # generate model\n",
    "            multiple_linreg = LinearRegression().fit(X_train_resampled, y_train_resampled)\n",
    "        elif resampled is None:\n",
    "            multiple_linreg = LinearRegression().fit(X_train, y_train)\n",
    "        # threshold\n",
    "        multiple_linreg_predictions = (multiple_linreg.predict(X_test) >= threshold) * 1.0\n",
    "        # calc score\n",
    "        if scoring=='accuracy':\n",
    "            score = metrics.accuracy_score(y_test, multiple_linreg_predictions)\n",
    "        elif scoring=='precision':\n",
    "            score = metrics.precision_score(y_test, multiple_linreg_predictions)\n",
    "        elif scoring=='f1':\n",
    "            score = metrics.f1_score(y_test, multiple_linreg_predictions)\n",
    "        elif scoring=='recall':\n",
    "            score = metrics.recall_score(y_test, multiple_linreg_predictions)\n",
    "        elif scoring=='roc_auc':\n",
    "            score = roc_auc_score(y_test, multiple_linreg_predictions)\n",
    "        # append to results\n",
    "        results.append(score)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now create an additional function which will perform 10 fold cross validation and store results in a dataframe. This will done in order simplify further analysis the dataset, looking at accuracy, precision, recall, f1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this code is from the sample solution provided\n",
    "def cross_val_LinReg_DF(X,y, threshold=0.5, resampled=None):\n",
    "    \"\"\"Function to perform cross validation and store results \n",
    "    in dataframe. Cross validation looks at accuracy, precision, \n",
    "    recall, f1. Returns a dataframe with results\"\"\"\n",
    "\n",
    "    linRegResults = {}\n",
    "    # metrics to test against\n",
    "    test_metrics = ['accuracy','precision','recall', 'f1', 'roc_auc']\n",
    "    \n",
    "    for metric in test_metrics:\n",
    "        # generate test results\n",
    "        result = cross_val_LinReg(X, y, cv=10, scoring=metric, threshold = threshold, resampled = resampled)\n",
    "        length = len(result)\n",
    "        # store result in dict\n",
    "        linRegResults[metric] = sum(result)/length #np.std(result)]\n",
    "\n",
    "    # create dataframe with results\n",
    "    LinRegDF = pd.DataFrame.from_dict(linRegResults, orient='index', columns=['Linear_Regression Average' ])\n",
    "    \n",
    "    return LinRegDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linRegDF = cross_val_LinReg_DF(X,y)\n",
    "print(f\"Mean results from 100 fold cross validation are:\")\n",
    "linRegDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the results are marginally lower than we saw above for the test sample. For example, recall for the test sample was 0.46 in comparison to 0.44 seen here. However, the precision has increased from 92% to 98%. These differing results are due to the fact that we have taken the mean of 100 sets of results. \n",
    "\n",
    "# TODO discuss sd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) Logistic Regression   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Train a logistic regression model using only the descriptive features selected from part 1 above.\n",
    "\n",
    "We will now use the same feature set selected above to perform a logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train aka fit, a model using all continuous and categorical features.\n",
    "multiple_logisticreg = LogisticRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the weights learned for each feature.\n",
    "print(\"\\nFeatures are: \\n\", X_train.columns)\n",
    "print(\"\\nCoeficients are: \\n\", multiple_logisticreg.coef_[0])\n",
    "print(\"\\nIntercept is: \\n\", multiple_logisticreg.intercept_)\n",
    "print(\"\\nFeatures and coeficients: \\n\", list(zip(X_train.columns, multiple_logisticreg.coef_[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation of results\n",
    "\n",
    "Before discussing the coefficients printed above, we will first briefly introduce logistic regression. \n",
    "\n",
    "- As with linear regression, in logistic regression we find the line of best fit. \n",
    "- However, we have seen above that the output of linear regression in a continious value and is not a probability. Instead, we manually thresholded the value in order to convert it into a binary classification.\n",
    "- In logistic regression however, the output is a probability between 0 and 1.\n",
    "- Briefly, the logistic function involves the following steps: \n",
    "    - The line of best fit is found.\n",
    "    - The equation of the line is passed through a function called the sigmoid function.\n",
    "    - The sigmoid function outputs a probability between 0 and 1.\n",
    "    - The model then applies a threshold to the probability. Values < 0.5 are cast to 0 and values >= 0.5 are cast to 1. The threshold can be manually adjusted. \n",
    "    \n",
    "- The formula for the logistic regression function is: \n",
    "$probability(target=1|descriptive\\_features)=logistic(w_0 + w_1 * feature_1 + w_2*feature_2 + ...+ w_n*feature_n)$ <br>\n",
    "where $logistic(x)$ is defined as: $logistic(x) = \\frac{e ^ x}{1 + e ^ x} = \\frac{1}{1+e^{-x}}$\n",
    "\n",
    "**Discussing the coefficients learned by the model:**\n",
    "- We can see above that the intercept is -0.487 (this is w_0 is the above formula). This is the value of y when x is zero. In other words it is where the model intercepts the x axis. \n",
    "- If all other coefficients were zero, the model result would be -0.487.\n",
    "- We can see the coefficients for each feature above. They are zipped together in a single list for ease of visualisation and interpretation.\n",
    "- As explained above when discussing the coefficients learned for linear regression, the value of each dummy is in relation to the reference value for that feature.  \n",
    "- The effect of each of these value 'x' is the same as for linear regression. When the value 'x' is fed into the logistic function we will see the difference between the two methods. \n",
    "- We mentioned above that linear regression is sensitive to outliers. Logistic regression on the other hand is good at dealing with outliers.\n",
    "\n",
    "## TODO add image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 Printing 100 predicted target features and evaluate the prediction\n",
    "- We will now print the predicted target feature value for the first 100 training samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_logisticreg_predictions_train = multiple_logisticreg.predict(X_train)\n",
    "\n",
    "print(\"\\nPredictions with multiple linear regression: \\n\")\n",
    "actual_vs_predicted_multiplelogisticreg = pd.concat([y_train, pd.DataFrame(multiple_logisticreg_predictions_train, columns=['Predicted'])], axis=1)\n",
    "print(actual_vs_predicted_multiplelogisticreg.head(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5 Evaluation metrics based on training data\n",
    "- We will print the classification evaluation measures computed on the training set (e.g. Accuracy, Confusion matrix, Precision, Recall, F1)\n",
    "- We will discuss finding based on these measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some more evaluation metrics.\n",
    "print(\"==================== Train Data =======================\")\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_train, multiple_logisticreg_predictions_train))\n",
    "print(\"Confusion matrix: \\n\", metrics.confusion_matrix(y_train, multiple_logisticreg_predictions_train))\n",
    "print(\"Classification report:\\n \", metrics.classification_report(y_train, multiple_logisticreg_predictions_train))\n",
    "auc_logistic_train = roc_auc_score(y_train, multiple_logisticreg_predictions_train)\n",
    "print(\"Area under the curve: \", auc_logistic_train)\n",
    "print(\"======================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation of results\n",
    "\n",
    "- As before, the **Confusion Matrix** tells us the following:\n",
    "    - [0][0] TRUE NEGATIVE - The number predicted 0 that was actually 0\n",
    "    - [0][1] FALSE POSITIVE - The number predicted 1 that is actually 0\n",
    "    - [1][0] FALSE NEGATIVE - The number predicted 0 that is actually 1\n",
    "    - [1][1] TRUE POSITIVE - The number predicted 1 that is actually 1\n",
    "    - We can see that we have 23 false negatives. As with linear regression, this value is high. \n",
    "- **Accuracy:** \n",
    "    - We have an accuracy of 96%\n",
    "- **Precision:**\n",
    "    - Precision negative is 97%\n",
    "    - Precision positive is 94%\n",
    "- **Recall:**\n",
    "    - Recall negative is 100%\n",
    "    - Recall positive is 59%\n",
    "- **F1 score:**\n",
    "    - F1 negative is 98%\n",
    "    - F1 positive is 73%\n",
    "\n",
    "Summary: \n",
    "- Given the results we acheived for our linear regression, these results seem reasonable.\n",
    "- The accuracy is in line with the 95% acheived for linear regression.\n",
    "- We can see that logistic regression has better scores for recall and f1 for the positive class than seen in linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6 Evaluate the model using the hold-out (30% examples) test set\n",
    "- We will now evaluate the model using the test set. \n",
    "- We will compare the results of the test set against the training set. \n",
    "- As described in section 2.7 above, will then compare the results to the results from a cross-validated model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will begin by printing the actual binary_outcome vs the predicted values based on the test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_logisticreg_predictions_test = multiple_logisticreg.predict(X_test)\n",
    "\n",
    "print(\"\\nPredictions with multiple linear regression: \\n\")\n",
    "actual_vs_predicted_multiplelogisticreg = pd.concat([y_test, pd.DataFrame(multiple_logisticreg_predictions_test, columns=['Predicted'])], axis=1)\n",
    "print(actual_vs_predicted_multiplelogisticreg.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now evaluate based on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some more evaluation metrics.\n",
    "print(\"==================== Test Data =======================\")\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_test, multiple_logisticreg_predictions_test))\n",
    "print(\"Confusion matrix: \\n\", metrics.confusion_matrix(y_test, multiple_logisticreg_predictions_test))\n",
    "print(\"Classification report:\\n \", metrics.classification_report(y_test, multiple_logisticreg_predictions_test))\n",
    "auc_logistic_test = roc_auc_score(y_test, multiple_logisticreg_predictions_test)\n",
    "print(\"Area under the curve: \", auc_logistic_test)\n",
    "print(\"==================== Train Data =======================\")\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_train, multiple_logisticreg_predictions_train))\n",
    "print(\"Confusion matrix: \\n\", metrics.confusion_matrix(y_train, multiple_logisticreg_predictions_train))\n",
    "print(\"Classification report:\\n \", metrics.classification_report(y_train, multiple_logisticreg_predictions_train))\n",
    "auc_logistic_train = roc_auc_score(y_train, multiple_logisticreg_predictions_train)\n",
    "print(\"Area under the curve: \", auc_logistic_train)\n",
    "print(\"======================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation of results and comparison with training set\n",
    "\n",
    "We will now compare the test and training results.\n",
    "- **Accuracy:** \n",
    "    - The accuracy of the test data very similar to the training set at 96%.\n",
    "- **Precision:** \n",
    "    - The precision score for predicting the negative class remains the same at 97%.\n",
    "    - The precision score for predicting the positive class has decreased from 94% to 88%. \n",
    "- **Recall:**\n",
    "    - The recall score for predicting the negative class has decreased from 100% to 99%.\n",
    "    - The recall score for predicting the positive class has increased from 59% to 62%.\n",
    "- **F1:**\n",
    "    - The f1 score for predicting the negative class has remained the same at 98%.\n",
    "    - The f1 score for predicting the positive class has remained the same at 73%.\n",
    "\n",
    "**Summary**\n",
    "- The difference between training and test results is very similar to the difference since in linear regression in section 2.6.\n",
    "- The precision positive has dropped slightly while the recall positive has increased slightly. These changes are normal as we are testing the model on data it has not seen before. \n",
    "- We will now investigate further with cross-validation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.7 Cross validation\n",
    "\n",
    "- We will now perform cross-validation, as explained above in section 2.7.\n",
    "- The first step we will take when performing cross-validation is to create a function which will perform 10 fold cross validation. This function will store the results into a dataframe. This step is taken in order to make it easier for us to evaluate the model by looking at accuracy, precision, recall, f1.\n",
    "- Unlike for linear regression, Scikit-Learn has a built in function called cross_val_score() which we will use to calculate the results of the cross-validation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO look at stdev also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method is from sample solution provided\n",
    "def cross_val_LogReg_DF(X,y, class_weight=None):\n",
    "    \"\"\"Function to perform cross validation and store results \n",
    "    in dataframe. Cross validation looks at accuracy, precision, \n",
    "    recall, f1. Returns a dataframe with results\"\"\"\n",
    "\n",
    "    logRegResults = {}\n",
    "    # metrics to test against\n",
    "    test_metrics = ['accuracy','precision','recall', 'f1', 'roc_auc']\n",
    "\n",
    "    for metric in test_metrics:\n",
    "        # generate test results\n",
    "        result = cross_val_score(LogisticRegression(class_weight=class_weight), X, y, scoring=metric, cv=10)\n",
    "        # store result in dict\n",
    "        logRegResults[metric] = result.mean() #np.std(result)]\n",
    "\n",
    "    # create dataframe with results\n",
    "    LogRegDF = pd.DataFrame.from_dict(logRegResults, orient='index', columns=['Logistic_Regression Average'])\n",
    "    \n",
    "    return LogRegDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below we will now call the function defined above and summarise the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logRegDF = cross_val_LogReg_DF(X,y)\n",
    "print(f\"Mean results from 10 fold cross validation are:\")\n",
    "logRegDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results are in line with the results seen for cross-validation in section 2.7. We can see that the results for recall and f1 are marginally lower than seen in the test set. The results for precision however, are higher at 95% compared to 88% in the test set. The accuracy is the same at 96%. These results make sense given that we have taken the mean of 10 sets of results. \n",
    "\n",
    "# TODO discuss sd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) Random Forest Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will now train a random forest model on our data.\n",
    "- As we are working on a classification problem, we will use classification trees. \n",
    "- We use the RandomForestClassifier() function. \n",
    "- The random state will be set to 1 to allow the results to be repeated. This means that any ties between features will be split in the same way each time we run the code, thus giving us the same tree each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train RF with 100 trees\n",
    "rfc = RandomForestClassifier(n_estimators=100, max_features='auto', oob_score=True, random_state=1)\n",
    "rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Train a random forest model using only the descriptive features selected from part 1 above.\n",
    "- A random forest model will now be trained on our test data.\n",
    "- A random forest is an ensemble of decision trees.  \n",
    "- **We will begin by explaining a Decision Tree:**\n",
    "    - A decision tree is a chart that shows probabilities. It is a set of if-then-else rules computed from the training data and then used to classify the test data. \n",
    "    - It begins at the top of the tree and splits the features based on the information gain. As default, Scikit-Learn measures this using the Gini index.\n",
    "        - As described at https://github.com/justmarkham/DAT4/blob/master/notebooks/15_decision_trees.ipynb, the gini index is a measure of total variance across classes in a region.\n",
    "        - A dataset containing only one class will have 0 Gini impurity. \n",
    "        - The idea when building a decision tree is to choose the feature with the least Gini impurity as the root node. \n",
    "     - Thus, the tree will always split on the feature that gives the highest information gain. The feature with the highest information gain (lowest Gini impurity) will sit at the root of each decision tree.\n",
    "    - We keep repeating this process until a stopping criteria is met (such as a **maximum depth of the tree** or the **minimum number of samples in the leaf**).\n",
    "    - The farthest branch on the tree represents the end result.\n",
    "- **Random Forest:**\n",
    "    - A random forest is made up of a number of decision trees. \n",
    "    - The output of the random forest is based on the majority output of all combined decision trees. \n",
    "    - The random forest will keep track of the importance of features in terms of the information gain described above. The importance will be explained further shortly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  4.2.1  Train Decision tree\n",
    "In order to demonstrate how decision trees work, we will now train 2 decision trees with the same data. \n",
    "- We will set one tree with max_depth=4 and the other with max_depth=10\n",
    "- Max depth determines how deep a decision tree can go before it must stop splitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc4 = DecisionTreeClassifier(max_depth=4, random_state=1)\n",
    "dtc10 = DecisionTreeClassifier(max_depth=10, random_state=1)\n",
    "dtc4.fit(X_train, y_train)\n",
    "dtc10.fit(X_train, y_train)\n",
    "print(\"Max depth 4: \\n\",dtc4)\n",
    "print(\"Max depth 10: \\n\",dtc10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  4.2.2  Display Decision tree\n",
    "We will create graph of each tree and store as an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Graphviz png\n",
    "with open(\"DecisionTree4.dot\", 'w') as f1:\n",
    "    f1 = export_graphviz(dtc4, out_file=f1, feature_names=X_train.columns)\n",
    "with open(\"DecisionTree10.dot\", 'w') as f2:\n",
    "    f2 = export_graphviz(dtc10, out_file=f2, feature_names=X_train.columns)\n",
    "!dot -Tpng DecisionTree4.dot -o DecisionTree4.png\n",
    "!dot -Tpng DecisionTree10.dot -o DecisionTree10.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"DecisionTree4.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"DecisionTree10.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wil now discuss the decision tree above with max_depth=:\n",
    "- We can see that the root is *IntakeType_Wildlife*. This was one of the dummies made for the feature *IntakeType*. \n",
    "- This feature splits the data best and has the highest information gain. We can see that one path is resolved from this one split alone. Animals who do not have a value <= 0.5 for *IntakeType_Wildlife* (and thus are Wildlife) result in a leaf node which is pure. That is, all samples are in the positive class. It is clear that this feature is the strongest predictor of the outcome.\n",
    "- For animals who are not wildlife, we can see that the tree continues to split at the next best feature each time until the stopping criteria of max_depth=4 is met. \n",
    "- As mentioned above, this is only one example of a decision tree. The random forest we have trained will be a combination of 100 decision trees. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.3 Interpretation of the random forest model\n",
    "\n",
    "Now that we have seen how decision trees work, we can now analyse the output from the random forest classifier which we invoked above. \n",
    "\n",
    "- As mentioned above, the random forest keeps track of the importance of features.\n",
    "- The list of importances are shown below. \n",
    "- The more important features are the features that the random forest has identified as having higher information gain. \n",
    "- The list is the combined result of the 100 decision trees which were used to train the random forest classifier. \n",
    "- This list will give us an idea of which features are the most important to keep as we move forward with our modelling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = pd.DataFrame({'feature': X_train.columns, 'importance':rfc.feature_importances_})\n",
    "importance.sort_values('importance', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can list from the list of importances that many features provide little information gain. Interestingly, we can see that the new features created at the end of homework1 are amongst the highest. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Printing 100 predicted target features and evaluate the prediction \n",
    "- Here we will print the predicted target feature value for the first 10 training examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted class labels for all examples, \n",
    "# using the trained model, on in-sample data (same sample used for training and test)\n",
    "rfc_predictions_train = rfc.predict(X_train)\n",
    "df_true_vs_rfc_predicted = pd.DataFrame({'ActualClass': y_train, 'PredictedClass': rfc_predictions_train})\n",
    "df_true_vs_rfc_predicted.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Evaluation metrics based on training data\n",
    "- We will print the classification evaluation measures computed on the training set (e.g. Accuracy, Confusion matrix, Precision, Recall, F1)\n",
    "- We will discuss finding based on these measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"==================== Train Data =======================\")\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_train, rfc_predictions_train))\n",
    "print(\"Confusion matrix: \\n\", metrics.confusion_matrix(y_train, rfc_predictions_train))\n",
    "print(\"Classification report:\\n \", metrics.classification_report(y_train, rfc_predictions_train))\n",
    "auc_random_forests_train = roc_auc_score(y_train, rfc_predictions_train)\n",
    "print(\"Area under the curve: \", auc_random_forests_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As before, we will begin by discussing the **Confusion Matrix**:\n",
    "    - We saw in linear and logistic regression that there was a high number of false negatives. \n",
    "    - Here, however, we only have 4 false negatives. This is a big improvement. \n",
    "- **Accuracy:** \n",
    "    - We have an accuracy of 99%\n",
    "- **Precision:**\n",
    "    - Precision negative is 99%\n",
    "    - Precision positive is 100%\n",
    "- **Recall:**\n",
    "    - Recall negative is 100%\n",
    "    - Recall positive is 93%\n",
    "- **F1 score:**\n",
    "    - F1 negative is 100%\n",
    "    - F1 positive is 96%\n",
    "    \n",
    "Summary: \n",
    "- We are very pleased with these results. Given the results we achieved for linear and logistic regression, these results show a large improvement in all areas, most notably in recall and f1 scores for the positive class. \n",
    "- However, we must evaluate these results on test data in order to see how the model is generalising. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Evaluate the model using the hold-out (30% examples) test set\n",
    "- These results from the test data will be compared the results from the training data.\n",
    "- In addition they will be compared to the results from a cross-validated model (i.e., a new model trained and evaluated using cross-validation on the full dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted class labels for all examples, \n",
    "# using the trained model, on in-sample data (same sample used for training and test)\n",
    "rfc_predictions_test = rfc.predict(X_test)\n",
    "df_true_vs_rfc_predicted_test = pd.DataFrame({'ActualClass': y_test, 'PredictedClass': rfc_predictions_test})\n",
    "df_true_vs_rfc_predicted_test.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"==================== Test Data =======================\")\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_test, rfc_predictions_test))\n",
    "print(\"Confusion matrix: \\n\", metrics.confusion_matrix(y_test, rfc_predictions_test))\n",
    "print(\"Classification report:\\n \", metrics.classification_report(y_test, rfc_predictions_test))\n",
    "auc_random_forests_test = roc_auc_score(y_test, rfc_predictions_test)\n",
    "print(\"Area under the curve: \", auc_random_forests_test)\n",
    "print(\"==================== Train Data =======================\")\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_train, rfc_predictions_train))\n",
    "print(\"Confusion matrix: \\n\", metrics.confusion_matrix(y_train, rfc_predictions_train))\n",
    "print(\"Classification report:\\n \", metrics.classification_report(y_train, rfc_predictions_train))\n",
    "auc_random_forests_train = roc_auc_score(y_train, rfc_predictions_train)\n",
    "print(\"Area under the curve: \", auc_random_forests_train)\n",
    "print(\"======================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation of results and comparison with training set\n",
    "\n",
    "We will now compare the test and training results.\n",
    "- **Accuracy:** \n",
    "    - The accuracy of the test data has decreased to 93%.\n",
    "- **Precision:** \n",
    "    - The precision score for predicting the negative class has decreased to 95%.\n",
    "    - The precision score for predicting the positive class has decreased from 100% to 62%. \n",
    "- **Recall:**\n",
    "    - The recall score for predicting the negative class has decreased from 100% to 97%.\n",
    "    - The recall score for predicting the positive class has increased from 93% to 54%.\n",
    "- **F1:**\n",
    "    - The f1 score for predicting the negative class has remained the same at 100% to 97%.\n",
    "    - The f1 score for predicting the positive class has remained the same at 96% to 58%.\n",
    "\n",
    "**Summary**\n",
    "\n",
    "- The accuracy for the test data is still high at 93%. However, most other scores have dropped considerably in comparison to the training data. \n",
    "- The results are reasonable considering that we are testing on data the model has not seen before. Most of the scores are in line with the scores seen on test data for linear and logistic regression. However, the precision for the positive class is lower. We will now perform cross-validation in order to check the results further. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 Cross validation\n",
    "- We will preform cross validation and store the results in a dictionary for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_RandomForest_DF(X,y, depth=None, estimators=100, class_weight=None):\n",
    "    \"\"\"Function to perform cross validation and store results \n",
    "    in dataframe. Cross validation looks at accuracy, precision, \n",
    "    recall, f1. Returns a dataframe with results\"\"\"\n",
    "\n",
    "    # store results in dict\n",
    "    RandomForestResults = {}\n",
    "    # metrics to test against\n",
    "    test_metrics = ['accuracy','precision','recall', 'f1', 'roc_auc']\n",
    "\n",
    "    for metric in test_metrics:\n",
    "        # generate test results\n",
    "        result = cross_val_score(RandomForestClassifier(n_estimators=estimators, max_features='auto', oob_score=True, random_state=1, max_depth=depth, class_weight=class_weight), X, y, scoring=metric, cv=10)\n",
    "        # store result in dict\n",
    "        RandomForestResults[metric] = result.mean() #np.std(result)\n",
    "    \n",
    "    # create dataframe with results\n",
    "    RandomForestDF = pd.DataFrame.from_dict(RandomForestResults, orient='index', columns=['Random_Forests Average'])\n",
    "\n",
    "    return RandomForestDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomForestDF = cross_val_RandomForest_DF(X,y)\n",
    "print(f\"Mean results from 10 fold cross validation are:\")\n",
    "RandomForestDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results are in line with the cross validation results for linear and logistic regression. The accuracy is similar to both models. Recall and f1 are similar to the logistic regression model. We can see however, that the precision is slightly lower than in both linear and logistic regression. We will compare all three models further in later sections. \n",
    "\n",
    "# TODO discuss sd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out-of-sample error\n",
    "\n",
    "To generate a random forest a subset of data is used to train each of the base models. As a result, some data does not get seen by the model during training. These samples can be used to evaluate the model. This is often an accurate way of evaluating the model as it is data the model has not seen. \n",
    "\n",
    "# TODO OOB in more detail\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the out-of-bag classification accuracy\n",
    "rfc.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result is in line with the cross validation results seen above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5) Improving predictive models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Which model of the ones trained above performs better at predicting the target feature? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO discuss how we are going to focus on recall for our specific case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the cell below we will display the cross-validation results of all three models. We will discuss the findings below the output. \n",
    "- By default, the results shown here are for predicting the positive class (binary_outcome = 1)\n",
    "- It is important to note that these results are for a subset of the original features, as we dropped many features in part 1.\n",
    "- Later in the assignment we will review the impact of removing these features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ResultsDF = pd.concat([linRegDF, logRegDF, RandomForestDF], axis=1)\n",
    "ResultsDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When looking at the average results, we can see that there are some slight variations in some of the scores. \n",
    "- Accuracy is identical in all models +/- 1%.\n",
    "- Precision is highest for linear regression and lowest for the random forest model.\n",
    "- Recall is lowest for linear regression and improves by 11% for both logistic regression and random forests. \n",
    "- F1 score is highest for logistic regression and lowest for linear regression.\n",
    "- Considering all scores, logistic regression appears to have performed the best on this sample of data.\n",
    "\n",
    "- In terms of standard deviation, linear regression has lower standard deviation scores than both logistic regression and random forest while random forest has the highest scores. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Is it more accurate than a simple model that always predicts the majority class?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will now compare our results to a simple model which predicts the majority class. \n",
    "- We know that class 0 is the majority class in our dataset. We will now run some code which confirms this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_count = y_test[y_test == 1].count()\n",
    "negative_count = y_test[y_test == 0].count()\n",
    "total = positive_count+ negative_count\n",
    "print(f'From original dataset: \\t\\tCount {total}')\n",
    "print(f'Total number class 1:\\t {positive_count}\\t{round(positive_count/len(y_test)*100,2)}%')\n",
    "print(f'Total number class 0:\\t {negative_count}\\t{round(negative_count/len(y_test)*100,2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate array of 300 zeros (length of test dataset)\n",
    "majority = np.zeros(300)\n",
    "# create dataframe \n",
    "df_majorityClass = pd.DataFrame(majority, columns=['prediction'])\n",
    "# calculate scores of simple predictin vs actual\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_test, df_majorityClass))\n",
    "print(\"Confusion matrix: \\n\", metrics.confusion_matrix(y_test, df_majorityClass))\n",
    "print(\"Classification report:\\n \", metrics.classification_report(y_test, df_majorityClass))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now discuss the results: \n",
    "# TODO\n",
    "\n",
    "just look at accuracy and precision - discuss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Summarise your understanding of the problem and of your predictive modeling results so far.\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can you think of any new ideas to improve the best model so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.1 Al features\n",
    "\n",
    "- In part 1 we created a new dataframe containing only a subset of features. \n",
    "- We preserved all features in the original dataframe. \n",
    "- We will now repeat the tests using all features.\n",
    "- The first step is to set up the datadrame with all features and to create dummies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe with all features = original dataframe\n",
    "df_all = pd.get_dummies(df)\n",
    "# set X,y\n",
    "y = df_all[\"binary_outcome\"]\n",
    "X = df_all.drop([\"binary_outcome\"],1)\n",
    "print(\"number of features incl dummies: \", len(X.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now that we have created dummies on the dataset of all features, we are ready to test the models.\n",
    "- Recall that in sections 2,3 and 4 above we created functions which return cross validation scores in dataframes.\n",
    "- We will now invoke those functions in order to obtain the scores for all features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate Linear regression, logistic regression, random forest results again using all features\n",
    "linRegDF = cross_val_LinReg_DF(X,y)\n",
    "logRegDF = cross_val_LogReg_DF(X,y)\n",
    "RandomForestDF = cross_val_RandomForest_DF(X,y)\n",
    "# merge all 3 models into dataframe\n",
    "ResultsDF_allFeatures = pd.concat([linRegDF, logRegDF, RandomForestDF], axis=1)\n",
    "ResultsDF_allFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ease of visualisation, we will now print out the scores for both the feature subset we have been working with and all features. We will discuss the results below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nScores using all features:\")\n",
    "print(\"===================================================================\")\n",
    "print(ResultsDF_allFeatures)\n",
    "print(\"\\nScores using subset of features:\")\n",
    "print(\"===================================================================\")\n",
    "print(ResultsDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO discuss values\n",
    "- **Linear Regression:** We can see that accuracy remained the same when the number of features was reduced. Precision increased by 8% while recall decreased by 11% and f1 decreased by 7%. \n",
    "- **Logistic Regression:** We can see that accuracy increased by 1% when the number of features was reduced. Precision increased significantly by 12%. Recall decreased by 6% while f1 remained the same. \n",
    "- **Random Forests:** Accuracy remained the same at 95% when features were removed. Precision decreased by 11%. Recall increased slightly and f1 decreased slightly. \n",
    "\n",
    "We will now perform some further steps to see if we can improve the models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further optomisations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further cleaning of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove CatOrDog and Breed as this information is represented by Wildlife. Proof:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['BreedOriginal'][df['IntakeType']=='Wildlife'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['IntakeType'][df['BreedOriginal']=='Other Animal Breeds'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['AnimalType_Intake'][df['IntakeType']=='Wildlife'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_featuresRemoved = df_version1.copy()\n",
    "df_featuresRemoved.drop(['CatOrDog_1'], 1, inplace=True)\n",
    "df_featuresRemoved = df_featuresRemoved[df_featuresRemoved.columns.drop(list(df_featuresRemoved.filter(regex='Breed')))]\n",
    "print('\\nRemaining columns:', df_featuresRemoved.columns)\n",
    "print('\\nNew shape:', df_featuresRemoved.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_featuresRemoved[\"binary_outcome\"]\n",
    "X = df_featuresRemoved.drop([\"binary_outcome\"],1)\n",
    "print(\"number of features incl dummies: \", len(X.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate Linear regression, logistic regression, random forest results again using all features\n",
    "linRegDF = cross_val_LinReg_DF(X,y)\n",
    "logRegDF = cross_val_LogReg_DF(X,y)\n",
    "RandomForestDF = cross_val_RandomForest_DF(X,y)\n",
    "# merge all 3 models into dataframe\n",
    "ResultsDF_featuresRemoved = pd.concat([linRegDF, logRegDF, RandomForestDF], axis=1)\n",
    "ResultsDF_featuresRemoved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that logistic regression and random forest have better scores than with all features. We will proceed with this new dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balancing the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will now try to balance the dataset just created. \n",
    "- We will first try with SMOTE, then using the class_weights parameter and will compare. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up x and y \n",
    "y = df_featuresRemoved['binary_outcome']\n",
    "X = df_featuresRemoved.drop([\"binary_outcome\"],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into two datasets: 70% training and 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=1, stratify=y)\n",
    "\n",
    "print(\"original range is: \",df_featuresRemoved.shape[0])\n",
    "print(\"training range (70%):\\t rows 0 to\", round(X_train.shape[0]))\n",
    "print(\"test range (30%): \\t rows\", round(X_train.shape[0]), \"to\", round(X_train.shape[0]) + X_test.shape[0])\n",
    "\n",
    "print(\"\\nTarget feature binary_outcome total dataset counts:\\n\\n\", y.value_counts())\n",
    "print(\"\\nTraining data binary_outcome counts:\\n\", y_train.value_counts())\n",
    "print(\"\\nTest data binary_outcome counts:\\n\", y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  https://github.com/scikit-learn-contrib/imbalanced-learn/issues/562\n",
    "\n",
    "def over_sample_minority(X, y, sampler=None):\n",
    "    if sampler is None:\n",
    "        sampler = SMOTENC(categorical_features=[], random_state=42)\n",
    "    X[\"temp\"] = 0\n",
    "    n_features = X.shape[1] - 1\n",
    "    indices = range(n_features)\n",
    "    sampler.set_params(categorical_features=indices)\n",
    "    X_resampled, y_resampled = sampler.fit_resample(X, y)\n",
    "    del X_resampled[\"temp\"]\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "X_train_resampled, y_train_resampled = over_sample_minority(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to reset the index to allow contatenation with predicted values otherwise not joining on same index...\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "y_test.reset_index(drop=True, inplace=True)\n",
    "X_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"original range is: \",df_version1.shape[0])\n",
    "print(\"training range (70%):\\t rows 0 to\", round(X_train_resampled.shape[0]))\n",
    "print(\"test range (30%): \\t rows\", round(X_train.shape[0]), \"to\", round(X_train.shape[0]) + X_test.shape[0])\n",
    "\n",
    "print(\"\\nTarget feature binary_outcome total dataset counts:\\n\\n\", y.value_counts())\n",
    "print(\"\\nTraining data binary_outcome counts:\\n\", y_train_resampled.value_counts())\n",
    "print(\"\\nTest data binary_outcome counts:\\n\", y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train aka fit, a model using all continuous and categorical features.\n",
    "multiple_linreg = LinearRegression().fit(X_train_resampled, y_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the prediction and threshold the value. If >= 0.5 its true\n",
    "multiple_linreg_predictions_train = (multiple_linreg.predict(X_train_resampled) >= 0.5) * 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the prediction and threshold the value. If >= 0.5 its true\n",
    "multiple_linreg_predictions_test = (multiple_linreg.predict(X_test) >= 0.5) * 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print evaluation metrics.\n",
    "print(\"==================== Test Data =======================\")\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_test, multiple_linreg_predictions_test))\n",
    "print(\"Confusion matrix: \\n\", metrics.confusion_matrix(y_test, multiple_linreg_predictions_test))\n",
    "print(\"Classification report - Test data:\\n \", metrics.classification_report(y_test, multiple_linreg_predictions_test))\n",
    "auc_linear_test = roc_auc_score(y_test, multiple_linreg_predictions_test)\n",
    "print(\"Area under the curve: \", auc_linear_test)\n",
    "print(\"\\n==================== Train Data ======================\")\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_train_resampled, multiple_linreg_predictions_train))\n",
    "print(\"Confusion matrix: \\n\", metrics.confusion_matrix(y_train_resampled, multiple_linreg_predictions_train))\n",
    "print(\"\\nClassification report: - Training data\\n \", metrics.classification_report(y_train_resampled, multiple_linreg_predictions_train))\n",
    "auc_linear_train = roc_auc_score(y_train_resampled, multiple_linreg_predictions_train)\n",
    "print(\"Area under the curve: \", auc_linear_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linRegDF_resampled = cross_val_LinReg_DF(X,y,0.5,\"resampled\")\n",
    "print(f\"Mean results from 100 fold cross validation after balancing the training data are:\")\n",
    "linRegDF_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression on balanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train aka fit, a model using all continuous and categorical features.\n",
    "multiple_logisticreg = LogisticRegression().fit(X_train_resampled, y_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_logisticreg_predictions_train = multiple_logisticreg.predict(X_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_logisticreg_predictions_test = multiple_logisticreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some more evaluation metrics.\n",
    "print(\"==================== Test Data =======================\")\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_test, multiple_logisticreg_predictions_test))\n",
    "print(\"Confusion matrix: \\n\", metrics.confusion_matrix(y_test, multiple_logisticreg_predictions_test))\n",
    "print(\"Classification report:\\n \", metrics.classification_report(y_test, multiple_logisticreg_predictions_test))\n",
    "auc_logistic_test = roc_auc_score(y_test, multiple_logisticreg_predictions_test)\n",
    "print(\"Area under the curve: \", auc_logistic_test)\n",
    "print(\"==================== Train Data =======================\")\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_train_resampled, multiple_logisticreg_predictions_train))\n",
    "print(\"Confusion matrix: \\n\", metrics.confusion_matrix(y_train_resampled, multiple_logisticreg_predictions_train))\n",
    "print(\"Classification report:\\n \", metrics.classification_report(y_train_resampled, multiple_logisticreg_predictions_train))\n",
    "auc_logistic_train = roc_auc_score(y_train_resampled, multiple_logisticreg_predictions_train)\n",
    "print(\"Area under the curve: \", auc_logistic_train)\n",
    "print(\"======================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train RF with 100 trees\n",
    "rfc = RandomForestClassifier(n_estimators=100, max_features='auto', oob_score=True, random_state=1)\n",
    "rfc.fit(X_train_resampled, y_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_predictions_train = rfc.predict(X_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_predictions_test = rfc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"==================== Test Data =======================\")\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_test, rfc_predictions_test))\n",
    "print(\"Confusion matrix: \\n\", metrics.confusion_matrix(y_test, rfc_predictions_test))\n",
    "print(\"Classification report:\\n \", metrics.classification_report(y_test, rfc_predictions_test))\n",
    "auc_random_forests_test = roc_auc_score(y_test, rfc_predictions_test)\n",
    "print(\"Area under the curve: \", auc_random_forests_test)\n",
    "print(\"==================== Train Data =======================\")\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_train_resampled, rfc_predictions_train))\n",
    "print(\"Confusion matrix: \\n\", metrics.confusion_matrix(y_train_resampled, rfc_predictions_train))\n",
    "print(\"Classification report:\\n \", metrics.classification_report(y_train_resampled, rfc_predictions_train))\n",
    "auc_random_forests_train_resampled = roc_auc_score(y_train_resampled, rfc_predictions_train)\n",
    "print(\"Area under the curve: \", auc_random_forests_train_resampled)\n",
    "print(\"======================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO discussion\n",
    "\n",
    "- based on change between lin reg test and cv, we conclude balancing not helpful.\n",
    "- Difficult to analyse using cross_val_score\n",
    "- Will now try balancing using class_weights parameter instead and compare to these results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class_weight parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now try to improve our models by setting the parameter class_weight to balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced = df_featuresRemoved.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_balanced[\"binary_outcome\"]\n",
    "X = df_featuresRemoved.drop([\"binary_outcome\"],1)\n",
    "print(\"Number of features: \", len(X.columns))\n",
    "\n",
    "# calculate Linear regression, logistic regression, random forest results again\n",
    "linRegDF = cross_val_LinReg_DF(X,y)\n",
    "logRegDF = cross_val_LogReg_DF(X,y, class_weight = 'balanced')\n",
    "RandomForestDF = cross_val_RandomForest_DF(X,y, class_weight='balanced')\n",
    "# merge all 3 models into dataframe\n",
    "ResultsDF_balanced = pd.concat([linRegDF, logRegDF, RandomForestDF], axis=1)\n",
    "ResultsDF_balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Linear regression does not have a parameter *class_weight* and so its values are unchanged. \n",
    "- We can see that the recall score for random forests has increased from 56% to 57% while the precision has remained at 79%. We are happy with this.\n",
    "- The recall for logistic regression has increased to 76% however, the precision has dropped to 44%. We will now try to find a better balance between the recall and precision scores. In order to do this we will manually enter the weights for the *class_weight* parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logRegDF_balanced = cross_val_LogReg_DF(X,y, class_weight = {0:0.2, 1:0.8})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ResultsDF_balanced_v2 = pd.concat([linRegDF, logRegDF_balanced, RandomForestDF], axis=1)\n",
    "ResultsDF_balanced_v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO \n",
    "\n",
    "Explain class weights\n",
    "discuss findings\n",
    "explain what to do if trying to improve precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression threshold tuning\n",
    "\n",
    "- We have seen throughout the assignment so far that we are obtaining a high number of false negatives. \n",
    "- This is turn is then causing our recall scores for the positive class to be low. \n",
    "- In order to try to overcome this, we will now run linear and logisitic regression again, but this time with the threshold set to different values, rather than 0.5.\n",
    "- This will allow us to catch more of the class 1. However, there is a risk that we will get more false positives. \n",
    "- We will use df_version1 for this analysis, which is the df we created in part 1. It has the low correlation features removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up x and y \n",
    "y = df_balanced['binary_outcome']\n",
    "X = df_balanced.drop([\"binary_outcome\"],1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will now change the threshold to 0.4. We will see if this improves our recall score. \n",
    "- We have added a parameter *threshold* to the cross_val_LinReg_DF() function in order to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Linear_Regression_Thresholds = pd.DataFrame()\n",
    "thresholds = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "for threshold in thresholds: \n",
    "    df_thresholds = cross_val_LinReg_DF(X,y, threshold)\n",
    "    df_thresholds.rename(columns={'Linear_Regression Average':f'thresold={threshold}'}, inplace=True)\n",
    "    Linear_Regression_Thresholds = pd.concat([Linear_Regression_Thresholds, df_thresholds], axis=1)\n",
    "\n",
    "Linear_Regression_Thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that a threshold of 3 gives us the best balance between precision and recall. \n",
    "- Here we get a recall score of 0.65 and a precision score of 0.75."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results so far: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linRegDF_thresholded = cross_val_LinReg_DF(X,y, 0.3)\n",
    "logRegDF_balanced = cross_val_LogReg_DF(X,y, class_weight = {0:0.35, 1:0.65})\n",
    "RandomForestDF_balanced = cross_val_RandomForest_DF(X,y, class_weight={0:0.25, 1:0.75})\n",
    "# merge all 3 models into dataframe\n",
    "ResultsDF_balanced_thresholded = pd.concat([linRegDF_thresholded, logRegDF_balanced, RandomForestDF_balanced], axis=1)\n",
    "ResultsDF_balanced_thresholded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optomise random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In order to optomise the random forest model we will look at the following: \n",
    "- Find the best performing max_depth of the decision tree while keeping the accuracy as high as possible. \n",
    "    - In part 4, we trained a random forest with the Scikit-Learn default max_depth=None\n",
    "    - This means that the nodes will keep splitting until all nodes are pure (all data in the node comes from the same class)\n",
    "    - Generally, the deeper a tree goes, the more complex the model becomes. This can lead to overfitting because the model learns the training data very well and finds it difficult to generalise to new data. \n",
    "    - We saw in section 4.6 that there was a significant decrease in scores between the training and test data. \n",
    "    - As a result, we will now explore the max_depth parameter in order to try and reduce overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup dataframe\n",
    "y = df_balanced[\"binary_outcome\"]\n",
    "X = df_balanced.drop([\"binary_outcome\"],1)\n",
    "\n",
    "# initialised empty daraframe\n",
    "RandomForest_Depth = pd.DataFrame()\n",
    "\n",
    "# calculate cross val score incrementing max depth by 1 each iteration\n",
    "# append results to dataframe\n",
    "for i in range(1,20):\n",
    "    df1 = cross_val_RandomForest_DF(X,y,i)\n",
    "    df1.rename(columns={'Random_Forests Average':f'depth={i}'}, inplace=True)\n",
    "    RandomForest_Depth = pd.concat([RandomForest_Depth, df1], axis=1)\n",
    "\n",
    "# same calculation but use no max depth this time\n",
    "no_max_depth = cross_val_RandomForest_DF(X,y, class_weight={0:0.25, 1:0.75})\n",
    "no_max_depth.rename(columns={'Random_Forests Average':f'no_max_depth'}, inplace=True)\n",
    "RandomForest_Depth = pd.concat([RandomForest_Depth, no_max_depth], axis=1)\n",
    "RandomForest_Depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that the optimum value for *max_depth* appears to be 18. With this value we get a recall of 58.7% and a precision score of 81.5%.\n",
    "- This will vary between datasets but it is useful to know that nothing is gained from using *max_depth=None*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup dataframe\n",
    "y = df_balanced[\"binary_outcome\"]\n",
    "X = df_balanced.drop([\"binary_outcome\"],1)\n",
    "\n",
    "# initialised empty daraframe\n",
    "RandomForest_Estimators = pd.DataFrame()\n",
    "\n",
    "n_estimators = [1, 2, 4, 8, 16, 32, 64, 100, 200]\n",
    "\n",
    "for item in n_estimators:\n",
    "    df1 = cross_val_RandomForest_DF(X,y,estimators=item, class_weight={0:0.25, 1:0.75})\n",
    "    df1.rename(columns={'Random_Forests Average':f'estimators={item}'}, inplace=True)\n",
    "    RandomForest_Estimators = pd.concat([RandomForest_Estimators, df1], axis=1)\n",
    "\n",
    "RandomForest_Estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the optimum number of estimators is 64. However, the increase is not significant and this value will change with new datasets so for simplicity will keep it at the default of 100. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define optimum features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup dataframe\n",
    "y = df_balanced[\"binary_outcome\"]\n",
    "X = df_balanced.drop([\"binary_outcome\"],1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1 , test_size=0.3)\n",
    "\n",
    "# Train RF with 100 trees\n",
    "rfc = RandomForestClassifier(n_estimators=100, max_features='auto', oob_score=True, random_state=1)\n",
    "rfc.fit(X_train, y_train)\n",
    "importance = pd.DataFrame({'feature': X_train.columns, 'importance':rfc.feature_importances_})\n",
    "importance.sort_values('importance', ascending=False, inplace=True)\n",
    "importance.set_index('feature',1, inplace=True)\n",
    "importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up empty list to store features\n",
    "list_features = []\n",
    "\n",
    "# set up empty dataframe to store results\n",
    "RandomForest_Features = pd.DataFrame()\n",
    "count=0\n",
    "\n",
    "# loop over 'importance' dataframe adding 1 feature per loop (will be the next most important feature)\n",
    "# each loop calculates the score for the current number of features in the list_features\n",
    "# Each loop appends result to RandomForest_Features dataframe\n",
    "for index, row in importance.iterrows():\n",
    "    # only calculate the top 22 features to keep runtime down\n",
    "    if count < 36:\n",
    "        list_features.append(index)\n",
    "        X = df_all[list_features]\n",
    "        df1 = cross_val_RandomForest_DF(X,y, depth=18, {0:0.25, 1:0.75})\n",
    "        df1.rename(columns={'Random_Forests Average':f'features={len(list_features)}'}, inplace=True)\n",
    "        RandomForest_Features = pd.concat([RandomForest_Features, df1], axis=1)\n",
    "        count+=1\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# to display all columns    \n",
    "pd.set_option('display.max_columns', 40)\n",
    "RandomForest_Features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final review of models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
